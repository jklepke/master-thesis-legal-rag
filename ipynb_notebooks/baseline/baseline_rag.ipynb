{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation of Baseline RAG\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Vector Database\n",
    "3. Querying the Vector Database\n",
    "4. Output of Baseline RAG Model\n",
    "5. Evaluations\n",
    "\n",
    "This code is adapted and based on the provided implementation of pixegami [https://github.com/pixegami/langchain-rag-tutorial/tree/main]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library ===\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from langdetect import detect, DetectorFactory\n",
    "import pycountry\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === LangChain Core ===\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document  # (Optional: doppelt zu obigem)\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# === LangChain Community Integrationen ===\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# === OpenAI / LangChain OpenAI ===\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Lokale Projektmodule ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import generate_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.llm_as_a_judge import run_llm_judge_parallel, run_llm_rejudge_parallel, calculate_and_visualize_scores_of_evaluation_scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables. Assumes that the project directory contains a .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "CHROMA_PATH = \"chroma_dbs/chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content: str) -> str:\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Define replacements: Marker â†’ Paragraph Break\n",
    "    replacements = {\n",
    "        \"Nichtamtliches Inhaltsverzeichnis\": \"\\n\\n\",\n",
    "        \"zum Seitenanfang\": \"\",\n",
    "        \"zurÃ¼ck\": \"\",\n",
    "        \"weiter\": \"\",\n",
    "        \"Impressum\": \"\",\n",
    "        \"Datenschutz\": \"\",\n",
    "        \"BarrierefreiheitserklÃ¤rung\": \"\",\n",
    "        \"Feedback-Formular\": \"\"\n",
    "    }\n",
    "\n",
    "    # Replace values\n",
    "    for old_text, new_text in replacements.items():\n",
    "        for element in soup.find_all(string=re.compile(re.escape(old_text), re.IGNORECASE)):\n",
    "            element.replace_with(element.replace(old_text, new_text))\n",
    "\n",
    "    # Cleaning of additional linebreaks and whitespaces\n",
    "    cleaned_content = soup.get_text(separator='\\n', strip=True)\n",
    "    cleaned_content = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_content)\n",
    "\n",
    "    return cleaned_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaning_diff(raw_text, cleaned_text, title):\n",
    "    \n",
    "    save_dir=\"../../data/extracted_contents\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    raw_path = os.path.join(save_dir, \"raw_contents\", f\"{title}_raw.txt\")\n",
    "    clean_path = os.path.join(save_dir, \"cleaned_contents\", f\"{title}_cleaned.txt\")\n",
    "\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_text)\n",
    "\n",
    "    with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(datapath: str, baseline: bool = False):\n",
    "    with open(datapath, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    entries = []\n",
    "    for category in [\"laws\", \"ordinances\"]:\n",
    "        for entry in data.get(category, []):\n",
    "            entry[\"category\"] = category\n",
    "            entries.append(entry)\n",
    "\n",
    "    if baseline:\n",
    "        for entry in tqdm(entries, desc=\"Loading documents\"):\n",
    "            title = entry.get(\"title\", \"Unknown Title\")\n",
    "            base_url = entry.get(\"base_url\", \"\")\n",
    "            category = entry[\"category\"]\n",
    "\n",
    "            if base_url:\n",
    "                loader = WebBaseLoader(base_url)\n",
    "                docs = loader.load()\n",
    "                for doc in docs:\n",
    "                    raw_content = doc.page_content\n",
    "                    cleaned_content = clean_text(raw_content)\n",
    "                    doc.page_content = cleaned_content\n",
    "\n",
    "                    save_cleaning_diff(raw_content, cleaned_content, title)\n",
    "                    doc.metadata.update({\"title\": title, \"category\": category})\n",
    "                    documents.append(doc)\n",
    "            else:\n",
    "                print(f\"Missing base URL for: {title}\")\n",
    "\n",
    "    else:\n",
    "        cleaned_dir = os.path.join(\"..\", \"..\", \"data\", \"extracted_contents\", \"cleaned_contents\")\n",
    "        expected_files = {}\n",
    "        for entry in entries:\n",
    "            title = entry[\"title\"]\n",
    "            expected_filename = f\"{title}_cleaned.txt\"\n",
    "            expected_files[expected_filename] = {\n",
    "                \"source\": entry.get(\"base_url\", \"\"),\n",
    "                \"title\": entry.get(\"title\", \"Unknown Title\"),\n",
    "                \"language\": entry.get(\"language\", \"unknown\"),\n",
    "                \"category\": entry.get(\"category\", \"unknown\")\n",
    "            }\n",
    "\n",
    "        for filename in tqdm(os.listdir(cleaned_dir), desc=\"Loading cleaned files\"):\n",
    "            if filename in expected_files:\n",
    "                filepath = os.path.join(cleaned_dir, filename)\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                metadata = expected_files[filename]\n",
    "                metadata[\"source_file\"] = filename\n",
    "\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents loaded.\")\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_to_jsonable(documents: list[Document]) -> list[dict]:\n",
    "    return [\n",
    "        {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        }\n",
    "        for doc in documents\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_documents_for_sparse_retrieval(documents: list[Document], chunk_size: int, chunk_overlap: int, optimization: str, baseline: bool = False):\n",
    "    filename = f\"{len(documents)}_documents_for_sparse_retrieval_{chunk_size}_{chunk_overlap}_{optimization}{'_baseline' if baseline else ''}.json\"\n",
    "    filepath = f\"../retrieval_inputs/stored_chunks_for_sparse_retrieval/{filename}\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    jsonable_docs = documents_to_jsonable(documents)\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(jsonable_docs, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_for_sparse_retrieval(json_path: str) -> list[Document]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    return [\n",
    "        Document(page_content=entry[\"page_content\"], metadata=entry[\"metadata\"])\n",
    "        for entry in raw_data\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store(datapath, chunk_size=256, chunk_overlap=32, baseline: bool = False, optimization: str = \"default\"):\n",
    "    documents = load_documents(datapath=datapath, baseline=baseline)\n",
    "    chunks = split_text(documents, chunk_size, chunk_overlap)\n",
    "    save_documents_for_sparse_retrieval(chunks, chunk_size, chunk_overlap, optimization, baseline)\n",
    "    chroma_path = save_to_chroma(chunks, chunk_size, chunk_overlap, baseline, optimization)\n",
    "    return chroma_path\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def token_length(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def split_text(documents: list[Document], chunk_size, chunk_overlap):\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    chunk_index = 1\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"chunk_id\"] = str(uuid.uuid4())\n",
    "        chunk.metadata[\"chunk_index\"] = chunk_index\n",
    "        chunk_index+= 1\n",
    "\n",
    "    if len(chunks) > 10:\n",
    "        document = chunks[10]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document], chunk_size, chunk_overlap, baseline, optimization, batch_size=100):\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        print(f\"Removing existing directory: {CHROMA_PATH}\")\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    if baseline: \n",
    "        chroma_path = f\"../chroma_dbs/chroma_chunksize{chunk_size}_overlap{chunk_overlap}_{str(uuid.uuid4())[:8]}_baseline\"\n",
    "    else: \n",
    "        chroma_path = f\"../chroma_dbs/chroma_chunksize{chunk_size}_overlap{chunk_overlap}_{str(uuid.uuid4())[:8]}_{optimization}\"\n",
    "\n",
    "    \n",
    "    # preprare embeddings \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    # initialize Chroma\n",
    "    db = Chroma(embedding_function=embeddings, persist_directory=chroma_path)\n",
    "    \n",
    "    db.similarity_search_with_relevance_scores\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"ðŸ”¢ Store Chunks with Embeddings\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        db.add_documents(batch)\n",
    "        \n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return chroma_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Example text pairs from German energy and regulatory law\n",
    "text_pairs = [\n",
    "    (\n",
    "        \"What measures must grid operators take in case of grid congestion?\",\n",
    "        \"Grid operators must initiate redispatch measures in transmission networks during congestion to ensure grid stability.\",\n",
    "        \"high\"\n",
    "    ),\n",
    "    (\n",
    "        \"What role does the Federal Network Agency play in the energy market?\",\n",
    "        \"The agency sets the fees, monitors the network connection, and conducts semi-annual spot checks on supply quality.\",\n",
    "        \"medium\"\n",
    "    ),\n",
    "    (\n",
    "        \"Who is obliged to pay the EEG surcharge?\",\n",
    "        \"The termination of an employment relationship in the public sector must be made in writing.\",\n",
    "        \"low\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize embeddings\n",
    "embedding_model_ada = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "embedding_model_3s = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for text1, text2, similarity_level in text_pairs:\n",
    "    emb1_ada = embedding_model_ada.embed_query(text1)\n",
    "    emb2_ada = embedding_model_ada.embed_query(text2)\n",
    "    sim_ada = cosine_similarity([emb1_ada], [emb2_ada])[0][0]\n",
    "\n",
    "    emb1_3s = embedding_model_3s.embed_query(text1)\n",
    "    emb2_3s = embedding_model_3s.embed_query(text2)\n",
    "    sim_3s = cosine_similarity([emb1_3s], [emb2_3s])[0][0]\n",
    "\n",
    "    results.append({\n",
    "        \"Similarity Category\": similarity_level,\n",
    "        \"Text 1\": text1,\n",
    "        \"Text 2\": text2,\n",
    "        \"Similarity (ada-002)\": round(sim_ada, 4),\n",
    "        \"Similarity (3-small)\": round(sim_3s, 4)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Querying of Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Du bist ein hilfreicher, juristischer KI-Assistent fÃ¼r Gesetzestexte im deutschen Energie- und Versorgungsbereich. \n",
    "Generiere eine kurze, prÃ¤zise, konsistente und vollstÃ¤ndige Gesamtantwort von max. 200 Tokens basierend auf folgenden Kontext: \n",
    "\n",
    "Frage:\n",
    "{question}\n",
    "---\n",
    "Kontext:\n",
    "{context}\n",
    "---\n",
    "Sprache in der geantwortet werden soll: \n",
    "{language}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_query_to_german_if_needed(query: str) -> str:\n",
    "    try:\n",
    "        detected_lang = detect(query)\n",
    "    except LangDetectException:\n",
    "        detected_lang = \"unknown\"\n",
    "\n",
    "    if detected_lang == \"de\":\n",
    "        return query\n",
    "    else: \n",
    "        translation_prompt = f\"Translate the following question accurately and correctly into German:\\n\\n{query}\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an AI Translator specialized in translating texts to German.\"},\n",
    "                    {\"role\": \"user\", \"content\": translation_prompt}],\n",
    "            temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while translating: {e}\")\n",
    "            return query \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduzierbare Ergebnisse\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "def detect_language_name(text):\n",
    "    try:\n",
    "        lang_code = detect(text)\n",
    "        # LÃ¤ndercode in Klartext-Sprache umwandeln\n",
    "        language = pycountry.languages.get(alpha_2=lang_code)\n",
    "        if language is not None:\n",
    "            return language.name  # z.B. 'German'\n",
    "        else:\n",
    "            return lang_code  # fallback\n",
    "    except LangDetectException:\n",
    "        return \"unbekannt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_database(chroma_path):\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    db = Chroma(persist_directory=chroma_path, embedding_function=embedding_function)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query_text, db, k=6):\n",
    "    if len(db) == 0:\n",
    "        return [], \"No documents available in the database.\"\n",
    "\n",
    "    query_de = translate_query_to_german_if_needed(query_text)\n",
    "    results = db.similarity_search_with_relevance_scores(query_de, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(results, query_text, model_name, temperature: float = 0.7):\n",
    "        \n",
    "    # Differentiation, whether a single string or a list of documents is passed\n",
    "    if isinstance(results, str):\n",
    "        context_text = results\n",
    "    elif isinstance(results, list):\n",
    "        context_text = \"\\n\\n---\\n\\n\".join(\n",
    "            doc.page_content if isinstance(doc, Document) else str(doc)\n",
    "            for doc in results\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format for 'results': expected str or list of Document\")\n",
    "    \n",
    "    detected_language = detect_language_name(query_text)\n",
    "   \n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text, language=detected_language)\n",
    "\n",
    "    model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, database, model_name=\"gpt-4o-mini\"):\n",
    "    \n",
    "    results = retrieve_documents(query, db=database)\n",
    "\n",
    "    response = generate_answer(results, query, model_name)\n",
    "    \n",
    "    if results and isinstance(results[0], tuple):\n",
    "        results = [doc for doc, _ in results]\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\") for doc in results]\n",
    "    retrieved_chunk_contexts = [doc.page_content for doc in results]\n",
    "    retrieved_chunk_ids = [doc.metadata.get(\"chunk_id\") for doc in results]\n",
    "    retrieved_chunk_indices = [doc.metadata.get(\"chunk_index\") for doc in results]\n",
    "\n",
    "    return response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Output of Baseline RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path_baseline = generate_data_store(datapath=\"../../data/laws_and_ordinances.json\", chunk_size=1024, chunk_overlap=128, baseline=True)\n",
    "\n",
    "print(chroma_path_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path_test = \"../chroma_dbs/chroma_chunksize512_overlap64_78bd09a8_baseline\"\n",
    "query = \"Welchen Anwendungsbereich umfasst Â§1 des ElektromobilitÃ¤tsgesetz - EmoG?\"\n",
    "database = load_vector_database(chroma_path=chroma_path_baseline)\n",
    "model_name = \"gpt-4o-mini\"  # or any other supported model\n",
    "\n",
    "response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline(query=query, database=database, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" oder Wegen oder Teilen von diesen,3.durch das Zulassen von Ausnahmen von ZufahrtbeschrÃ¤nkungen oder Durchfahrtverboten,4.im Hinblick auf das Erheben von GebÃ¼hren fÃ¼r das Parken auf Ã¶ffentlichen StraÃŸen oder Wegen.(5) In Rechtsverordnungen nach Â§ 6 Absatz 1\"\n",
    "print(\"Token-Anzahl:\", token_length(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(f\"Query: {query} \\n\")\n",
    "print(f\"Response: {response} \\n\")\n",
    "print(f\"Sources: {sources} \\n\")\n",
    "print(f\"Retrieved Chunk Contexts: {retrieved_chunk_contexts} \\n\")\n",
    "print(f\"Retrieved Chunk Ids: {retrieved_chunk_ids} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = generate_evalset(chroma_db=chroma_path_baseline, test_set_size=50, \n",
    "                 query_distribution={\"single\": 0.6, \"multi_specific\": 0.2, \"multi_intra_document\": 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = \"eval_datasets/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_baseline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Embedding function\n",
    "def embed_text(text):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    # Returns a 2D tensor (list of lists)\n",
    "    return np.array(response.data[0].embedding).reshape(1, -1)\n",
    "\n",
    "# Evaluation of a single entry\n",
    "def evaluate_ground_truth_alignment(entry):\n",
    "    query = entry[\"query\"]\n",
    "    ground_truth = entry[\"ground_truth\"]\n",
    "    context = \" \".join(entry[\"ground_truth_chunk_contexts\"])\n",
    "\n",
    "    query_emb = embed_text(query)\n",
    "    answer_emb = embed_text(ground_truth)\n",
    "    context_emb = embed_text(context)\n",
    "\n",
    "    sim_query_answer = cosine_similarity(query_emb, answer_emb)[0][0]\n",
    "    sim_answer_context = cosine_similarity(answer_emb, context_emb)[0][0]\n",
    "\n",
    "    return {\n",
    "        \"query_id\": entry[\"query_id\"],\n",
    "        \"sim_query_answer\": round(sim_query_answer, 4),\n",
    "        \"sim_answer_context\": round(sim_answer_context, 4),\n",
    "        \"avg_alignment_score\": round((sim_query_answer + sim_answer_context) / 2, 4),\n",
    "        \"query\": query,\n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    "\n",
    "# Evaluation of the entire dataset + storage Evaluation of a single entry\n",
    "def evaluate_eval_dataset(eval_dataset_path, csv_path=\"eval_datasets/ground_truth_evaluation.csv\"):\n",
    "    \n",
    "    \n",
    "    with open(eval_dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    for entry in tqdm(eval_dataset, desc=\"â†’ Evaluating Ground Truth Alignment\"):\n",
    "        result = evaluate_ground_truth_alignment(entry)\n",
    "        results.append(result)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_query_answer = np.mean([r[\"sim_query_answer\"] for r in results])\n",
    "    avg_answer_context = np.mean([r[\"sim_answer_context\"] for r in results])\n",
    "    avg_total = np.mean([r[\"avg_alignment_score\"] for r in results])\n",
    "\n",
    "    print(\"\\nâ†’ Average Scores:\")\n",
    "    print(\"Query_Answer Similarity:\", round(avg_query_answer, 4))\n",
    "    print(\"Answer_Chunkcontext Similarity:\", round(avg_answer_context, 4))\n",
    "    print(\"Avg Alignment Score:\", round(avg_total, 4))\n",
    "\n",
    "    # Write only the average values to the CSV file\n",
    "    fieldnames = [\"avg_sim_query_answer\", \"avg_sim_answer_context\", \"avg_alignment_score\"]\n",
    "    with open(csv_path, mode=\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow({\n",
    "            \"avg_sim_query_answer\": round(avg_query_answer, 4),\n",
    "            \"avg_sim_answer_context\": round(avg_answer_context, 4),\n",
    "            \"avg_alignment_score\": round(avg_total, 4)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nâœ… Results were saved under: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_eval_dataset(eval_dataset_path=eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrich Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_rag_responses(eval_dataset, chroma_path, model_name=\"gpt-4o-mini\"):\n",
    "    \n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "    \n",
    "    for entry in tqdm(eval_dataset_json, desc=\"Processing RAG responses\"):\n",
    "        query = entry[\"query\"]\n",
    "\n",
    "        # Run RAG pipeline\n",
    "        response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline(query, db, model_name=model_name)\n",
    "\n",
    "        # Add new fields to file\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "        enriched_dataset.append(entry)\n",
    "\n",
    "    output_path = f\"{eval_dataset.replace('.json', '')}_rag_enriched.json\"\n",
    "    # Store results as new json file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path_baseline = \"../../chroma_dbs/chroma_chunksize1024_overlap128_c800ccc6_baseline\"\n",
    "\n",
    "enriched_evalset = enrich_eval_dataset_with_rag_responses(eval_dataset=eval_dataset, \n",
    "                                       chroma_path = chroma_path_baseline, \n",
    "                                       model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_evalset = \"eval_datasets/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_baseline_rag_enriched.json\"\n",
    "model_name=\"baseline_rag_1024_128\"\n",
    "\n",
    "retrieval_result = run_retrieval_evaluation(json_filename=enriched_evalset.split(\"/\")[-1], \n",
    "                                            model_name=model_name,\n",
    "                                            evaluation_mode=\"final_eval\"\n",
    "                                            )\n",
    "display(retrieval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_results = run_generation_evaluation(json_filename=enriched_evalset.split(\"/\")[-1], \n",
    "                                               model_name=model_name,\n",
    "                                               evaluation_mode=\"final_eval\"\n",
    "                                               ) \n",
    "display(generation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Generation on Golden Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = \"../../data/golden_evalset/golden_qa_evalset_generation.json\"\n",
    "chroma_path_baseline = \"../chroma_dbs/chroma_chunksize1024_overlap128_c800ccc6_baseline\"\n",
    "\n",
    "enriched_golden_evalset = enrich_eval_dataset_with_rag_responses(eval_dataset=golden_dataset, \n",
    "                                       chroma_path = chroma_path_baseline, \n",
    "                                       model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"baseline_rag_vector_golden_qa_set\"\n",
    "\n",
    "generation_results_golden_dataset = run_generation_evaluation(json_filename=enriched_golden_evalset.split(\"/\")[4], \n",
    "                                                              model_name=model_name, \n",
    "                                                              evaluation_mode=\"final_eval\") \n",
    "display(generation_results_golden_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge for Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"eval_datasets/golden_qa_evalset_generation_rag_enriched.json\"\n",
    "first_output_path = \"eval_results/golden_qa_evalset_baseline_rag_llm_as_a_judge_first_results.json\"\n",
    "final_rejudge_output_path = \"eval_results/golden_qa_evalset_baseline_rag_llm_as_a_judge_final_rejudge_results.json\"\n",
    "max_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge for Comparison and Further Justification\n",
    "\n",
    "llm_as_a_judge_first_eval_results_path = run_llm_judge_parallel(input_path=input_path, output_path=first_output_path, max_workers=max_workers)\n",
    "llm_as_a_judge_rejudge_results_path = run_llm_rejudge_parallel(input_path=llm_as_a_judge_first_eval_results_path, output_path=final_rejudge_output_path, max_workers=max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name_LLMaaJ_first = \"llm_as_a_judge_first_results\"\n",
    "output_file_name_LLMaaJ_rejudge = \"llm_as_a_judge_rejudge_results\"\n",
    "\n",
    "eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(manual_results_path, output_file_name_manual)\n",
    "llm_as_a_judge_first_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_first_eval_results_path, output_file_name_LLMaaJ_first)\n",
    "llm_as_a_judge_final_rejudge_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_rejudge_results_path, output_file_name_LLMaaJ_rejudge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
