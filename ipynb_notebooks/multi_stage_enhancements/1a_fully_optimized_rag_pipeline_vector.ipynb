{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a550bd",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation of Fully Optimized RAG - Vector Approach\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Vector Database\n",
    "3. Querying the Vector Database\n",
    "4. Output of Optimized RAG Pipelines\n",
    "5. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2263d1",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de274a85",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568a9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\Documents\\LEGAL-RAG\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\jonas\\Documents\\LEGAL-RAG\\ipynb_notebooks\\evaluation_datasets\\generation_eval\\generation_metrics.py:57: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library ===\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm, trange\n",
    "from typing import List, Union, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# === LangChain Core ===\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document  # (Optional: doppelt zu obigem)\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, TFIDFRetriever, EnsembleRetriever\n",
    "\n",
    "# === LangChain Community Integrationen ===\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# === OpenAI / LangChain OpenAI ===\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Lokale Projektmodule ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import (\n",
    "    clean_text,\n",
    "    save_documents_for_sparse_retrieval,\n",
    "    load_documents_for_sparse_retrieval,\n",
    "    save_to_chroma, \n",
    "    translate_query_to_german_if_needed,\n",
    "    detect_language_name,\n",
    "    load_vector_database,\n",
    "    generate_answer\n",
    ")\n",
    "\n",
    "from ipynb_notebooks.single_stage_enhancements.rankGPT_rerank import rankgpt_rerank\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import generate_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.llm_as_a_judge import run_llm_judge_parallel, run_llm_rejudge_parallel, calculate_and_visualize_scores_of_evaluation_scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72fe81",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables. Assumes that the project directory contains a .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma_dbs/chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0412985",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragraph_documents(datapath: str):\n",
    "    # Load JSON file\n",
    "    with open(datapath, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    documents = []\n",
    "    chunk_index = 1  # Track chunk index globally\n",
    "\n",
    "    for category in [\"laws\", \"ordinances\"]:\n",
    "        entries = data.get(category, [])\n",
    "        for entry in tqdm(entries, desc=f\"→ Processing {category}\"):\n",
    "            title = entry.get(\"title\", \"Unknown Title\")\n",
    "            paragraphs = entry.get(\"paragraphs\", [])\n",
    "\n",
    "            for para in tqdm(paragraphs, desc=f\"  ↳ Paragraphs in '{title}'\", leave=False):\n",
    "                para_url = para.get(\"paragraph_url\", \"\")\n",
    "                para_name = para.get(\"paragraph_name\", \"Unknown Paragraph\")\n",
    "\n",
    "                if para_url:\n",
    "                    try:\n",
    "                        # Load content from paragraph URL\n",
    "                        loader = WebBaseLoader(para_url)\n",
    "                        docs = loader.load()\n",
    "\n",
    "                        for doc in docs:\n",
    "                            raw_content = doc.page_content\n",
    "                            cleaned_content = clean_text(raw_content)\n",
    "                            doc.page_content = cleaned_content\n",
    "\n",
    "                            doc.metadata.update({\n",
    "                                \"law_title\": title,\n",
    "                                \"category\": category,\n",
    "                                \"paragraph_id\": para.get(\"paragraph_ID\"),\n",
    "                                \"paragraph_name\": para_name,\n",
    "                                \"paragraph_url\": para_url,\n",
    "                                \"chunk_id\": str(uuid.uuid4()),\n",
    "                                \"chunk_index\": chunk_index,\n",
    "                            })\n",
    "\n",
    "                            documents.append(doc)\n",
    "                            chunk_index += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading paragraph from URL {para_url}: {e}\")\n",
    "                else:\n",
    "                    print(f\"No paragraph URL found for {title}\")\n",
    "\n",
    "    if not documents:\n",
    "        raise ValueError(\"No paragraph documents could be loaded from the input.\")\n",
    "\n",
    "    print(f\"Successfully loaded {len(documents)} paragraph-level documents.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f4759",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector Database with Paragraph-Wise Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store_from_paragraphs(datapath: str, chunk_size: str = \"paragraph_wise_chunking\", chunk_overlap: str = \"no_overlap\", baseline: bool = False, optimization: str = \"fully_optimized_rag_pipeline_vector\"):\n",
    "    documents = load_paragraph_documents(datapath)\n",
    "    save_documents_for_sparse_retrieval(documents, chunk_size, chunk_overlap, optimization, baseline)\n",
    "    chroma_path = save_to_chroma(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap, baseline=baseline, optimization=optimization)\n",
    "    return chroma_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e99b1",
   "metadata": {},
   "source": [
    "### 3. Querying of Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14aaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents(\n",
    "    results: Union[List[Document], List[Tuple[Document, float]]],\n",
    "    query: str,\n",
    "    score_threshold: float = 0.25\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filters documents based on relevance scores or cosine similarity using normalized scores.\n",
    "\n",
    "    Args:\n",
    "        results: List of Documents or (Document, Score) tuples.\n",
    "        query: The search query string.\n",
    "        score_threshold: Normalized similarity threshold between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        List of Documents that pass the similarity threshold.\n",
    "    \"\"\"\n",
    "    filtered_docs = []\n",
    "\n",
    "    # Case 1: Scores are already provided\n",
    "    if results and isinstance(results[0], tuple):\n",
    "        scores = [score for _, score in results]\n",
    "        min_score, max_score = min(scores), max(scores)\n",
    "\n",
    "        for doc, score in results:\n",
    "            # Normalize score\n",
    "            norm_score = (score - min_score) / (max_score - min_score + 1e-8)\n",
    "            if norm_score >= score_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "    else:\n",
    "        docs = results\n",
    "        doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "        embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        # Embed query and documents\n",
    "        query_vec = embedding_model.embed_query(text=query)\n",
    "        doc_vecs = embedding_model.embed_documents(texts=doc_texts)\n",
    "\n",
    "        # Compute similarity\n",
    "        similarity_scores = cosine_similarity([query_vec], doc_vecs)[0]\n",
    "\n",
    "        # Normalize\n",
    "        min_score, max_score = similarity_scores.min(), similarity_scores.max()\n",
    "\n",
    "        for doc, score in zip(docs, similarity_scores):\n",
    "            norm_score = (score - min_score) / (max_score - min_score + 1e-8)\n",
    "            if norm_score >= score_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "    return filtered_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe63598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query_text, vectordb, chunk_documents_path: str, k: int, thresh_hold: float = 0.25):\n",
    "    if len(vectordb) == 0:\n",
    "        return [], \"No documents available in the database.\"\n",
    "\n",
    "    query_de = translate_query_to_german_if_needed(query_text)\n",
    "    documents = []  \n",
    "    \n",
    "    # Hybrid-Retriever Method Dense + TF-IDF with EnsembleRetriever\n",
    "    documents = load_documents_for_sparse_retrieval(chunk_documents_path)       \n",
    "\n",
    "    vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": 25}, search_type=\"similarity\")\n",
    "    result_documents_MRR = vectordb.max_marginal_relevance_search(query=query_de, k=25, fetch_k=50)\n",
    "    \n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 25\n",
    "    result_documents_BM25 = bm25_retriever.get_relevant_documents(query_de)\n",
    "\n",
    "    result_documents = []\n",
    "    result_documents.append(result_documents_MRR)\n",
    "    result_documents.append(result_documents_BM25)\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[vectordb_retriever, bm25_retriever], weights=[0.5, 0.5])\n",
    "    result_documents = ensemble_retriever.weighted_reciprocal_rank(result_documents)\n",
    "    result_documents = result_documents[:k]\n",
    "    \n",
    "    # Post-Retrieval Optimization: Filtering Documents upon Relevancy\n",
    "    filtered_documents = filter_documents(results=result_documents, query=query_de, score_threshold=thresh_hold)\n",
    "    \n",
    "    # Post-Retrieval Optimization: Reranking Retrieved Documents with RankGPT\n",
    "    reranked_documents = rankgpt_rerank(query_de, filtered_documents, model_name=\"gpt-4o-mini\", window_size=4, step=1)\n",
    "    \n",
    "    return reranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_consistent_answer(answers):\n",
    "    embeddings = embedding_model.embed_documents(texts=answers)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    avg_sim = similarity_matrix.mean(axis=1)\n",
    "    best_index = int(np.argmax(avg_sim))\n",
    "    return answers[best_index], avg_sim, similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_answer_similarity_heatmap(\n",
    "    answers: list[str],\n",
    "    labels: list[str] = None,\n",
    "    similarity_matrix: Optional[np.ndarray] = None,\n",
    "    avg_similarities: Optional[np.ndarray] = None,\n",
    "    title: str = \"LLM Answer Similarity\"\n",
    "):\n",
    "\n",
    "    if similarity_matrix is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = model.encode(answers)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Model {i+1}\" for i in range(len(answers))]\n",
    "\n",
    "    if avg_similarities is not None:\n",
    "        annotated_labels = [\n",
    "            f\"{name}\\nØ={avg:.4f}\" for name, avg in zip(labels, avg_similarities)\n",
    "        ]\n",
    "    else:\n",
    "        annotated_labels = labels\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    sns.heatmap(\n",
    "        similarity_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".4f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=annotated_labels,\n",
    "        square=True,\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"LLM Answer\")\n",
    "    plt.ylabel(\"LLM Answer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_optimized_rag_pipeline_vector(\n",
    "    query,\n",
    "    database,\n",
    "    chunk_documents_path: str,\n",
    "    k=6,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    n_consistency=5,\n",
    "    temperature=0.7,\n",
    "    heatmap=False\n",
    "):\n",
    "    # Initializing\n",
    "    retrieved_contexts = []\n",
    "    retrieved_sources = []\n",
    "    retrieved_ids = []\n",
    "    retrieved_indices = []\n",
    "    retrieved_ids_set = set()\n",
    "\n",
    "    # Retrieve documents from vector\n",
    "    results = retrieve_documents(\n",
    "        query_text=query,\n",
    "        vectordb=database,\n",
    "        chunk_documents_path=chunk_documents_path,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "\n",
    "    for doc in results:\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\")\n",
    "        if chunk_id not in retrieved_ids_set:\n",
    "            retrieved_contexts.append(doc.page_content)\n",
    "            retrieved_sources.append(doc.metadata.get(\"source\"))\n",
    "            retrieved_ids.append(chunk_id)\n",
    "            retrieved_indices.append(doc.metadata.get(\"chunk_index\"))\n",
    "            retrieved_ids_set.add(chunk_id)\n",
    "            \n",
    "\n",
    "    # shorten retrieved context to 3 docs due to high Recall, MMR & MAP, so that few irrelevant noise is added to the answer generation\n",
    "    shortened_results = [doc for doc in results[:3]]\n",
    "\n",
    "    # Self-Consistency Generation    \n",
    "    answers = []\n",
    "    for i in range(n_consistency):\n",
    "        try:\n",
    "            answer = generate_answer(results=shortened_results, query_text=query, model_name=model_name, temperature=temperature)\n",
    "            answers.append(answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at answer no. {i+1}: {e}\")\n",
    "            answers.append(\"\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    response, similarities, similarity_matrix = select_most_consistent_answer(answers)\n",
    "\n",
    "    if heatmap:\n",
    "        print(\"\\n--- Self-Consistency Antworten ---\")\n",
    "        for i, a in enumerate(answers):\n",
    "            print(f\"[{i+1}] ({similarities[i]:.4f}): {a}\")\n",
    "        plot_answer_similarity_heatmap(\n",
    "            answers=answers,\n",
    "            labels=[f\"Ans {i+1}\" for i in range(n_consistency)],\n",
    "            similarity_matrix=similarity_matrix,\n",
    "            avg_similarities=similarities\n",
    "        )\n",
    "\n",
    "    return response.strip(), retrieved_sources, retrieved_contexts, retrieved_ids, retrieved_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b49b2",
   "metadata": {},
   "source": [
    "### 4. Output of Baseline RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path_fully_optimized_rag_pipeline_vector = generate_data_store_from_paragraphs(datapath=\"../../data/laws_and_ordinances.json\")\n",
    "\n",
    "print(chroma_path_fully_optimized_rag_pipeline_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Welchen Anwendungsbereich umfasst §1 des Elektromobilitätsgesetz - EmoG?\"\n",
    "chroma_path_fully_optimized_rag_pipeline_vector = \"../chroma_dbs/chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector\"\n",
    "database = load_vector_database(chroma_path=chroma_path_fully_optimized_rag_pipeline_vector)\n",
    "chunk_documents_path= \"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_vector.json\"\n",
    "model_name = \"gpt-4o-mini\"  # or any other supported model\n",
    "\n",
    "response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = fully_optimized_rag_pipeline_vector(query=query, \n",
    "                                                                                                                                database=database,\n",
    "                                                                                                                                chunk_documents_path=chunk_documents_path,\n",
    "                                                                                                                                model_name=model_name,\n",
    "                                                                                                                                heatmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7994395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(f\"Query: {query} \\n\")\n",
    "print(f\"Response: {response} \\n\")\n",
    "print(f\"Sources: {sources} \\n\")\n",
    "print(f\"Retrieved Chunk Contexts: {retrieved_chunk_contexts} \\n\")\n",
    "print(f\"Retrieved Chunk Ids: {retrieved_chunk_ids} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fc677",
   "metadata": {},
   "source": [
    "### 4. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b548033",
   "metadata": {},
   "source": [
    "#### Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = generate_evalset(chroma_db=chroma_path_fully_optimized_rag_pipeline_vector, test_set_size=50, \n",
    "                 query_distribution={\"single\": 0.6, \"multi_specific\": 0.2, \"multi_intra_document\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0eaac9",
   "metadata": {},
   "source": [
    "#### Enrich Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_single_entry(entry, db, chunk_documents_path, model_name):\n",
    "    query = entry[\"query\"]\n",
    "    \n",
    "    try:\n",
    "        response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = fully_optimized_rag_pipeline_vector(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            chunk_documents_path=chunk_documents_path,\n",
    "            model_name=model_name\n",
    "        )\n",
    "\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "    except Exception as e:\n",
    "        entry[\"generated_response\"] = f\"Fehler: {str(e)}\"\n",
    "        entry[\"retrieved_chunk_contexts\"] = []\n",
    "        entry[\"retrieved_chunk_ids\"] = []\n",
    "        entry[\"retrieved_chunk_indices\"] = []\n",
    "\n",
    "    # Optional: Delay between requests to stay under rate limits\n",
    "    time.sleep(1.2)\n",
    "    return entry\n",
    "\n",
    "def enrich_eval_dataset_with_fully_optimized_rag_responses_vector_parallel(\n",
    "    eval_dataset,\n",
    "    chroma_path,\n",
    "    chunk_documents_path,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    max_workers=20\n",
    "):\n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_single_entry, entry, db, chunk_documents_path, model_name): entry\n",
    "            for entry in eval_dataset_json\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing RAG responses (parallel)\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:  # Skip None if any errors are handled in process_single_entry\n",
    "                    enriched_dataset.append(result)\n",
    "            except Exception as e:\n",
    "                entry = futures[future]\n",
    "                print(f\"Fehler bei query_id={entry.get('query_id')}: {e}\")\n",
    "\n",
    "    # Sort the dataset by query_id\n",
    "    sorted_enriched_dataset = sorted(enriched_dataset, key=lambda x: x.get(\"query_id\", 0))\n",
    "\n",
    "    output_path = f\"{eval_dataset.replace('.json', '')}_rag_enriched.json\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted_enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = \"eval_datasets/artificial_evaluation_dataset_for_chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector.json\"\n",
    "chroma_path_fully_optimized_rag_pipeline_vector = \"../chroma_dbs/chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector\"\n",
    "chunk_documents_path = \"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_vector.json\"\n",
    "\n",
    "enriched_evalset = enrich_eval_dataset_with_fully_optimized_rag_responses_vector_parallel(eval_dataset=eval_dataset, \n",
    "                                       chroma_path = chroma_path_fully_optimized_rag_pipeline_vector, \n",
    "                                       chunk_documents_path=chunk_documents_path,\n",
    "                                       model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058553c7",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04197a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = \"eval_datasets/artificial_evaluation_dataset_for_chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector.json\"\n",
    "# enriched_evalset = \"eval_datasets/artificial_evaluation_dataset_for_chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector_rag_enriched_saved2.json\"\n",
    "chroma_path_fully_optimized_rag_pipeline_vector = \"../chroma_dbs/chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector\"\n",
    "chunk_documents_path = \"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_vector.json\"\n",
    "model_name=\"fully_optimized_rag_pipeline_vector\"\n",
    "\n",
    "retrieval_result = run_retrieval_evaluation(json_filename=enriched_evalset.split(\"/\")[-1], \n",
    "                                            model_name=model_name,\n",
    "                                            evaluation_mode=\"final_eval\"\n",
    "                                            )\n",
    "display(retrieval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0afa41",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_results = run_generation_evaluation(json_filename=enriched_evalset.split(\"/\")[-1], \n",
    "                                               model_name=model_name, \n",
    "                                               evaluation_mode=\"final_eval\"\n",
    "                                               ) \n",
    "display(generation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enriched_evalset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8589cd",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Generation on Golden Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45192d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = \"eval_datasets/golden_qa_evalset_generation.json\"\n",
    "chroma_path_fully_optimized_rag_pipeline_vector = \"../chroma_dbs/chroma_chunksizeparagraph_wise_chunking_overlapno_overlap_56f329f9_fully_optimized_rag_pipeline_vector\"\n",
    "chunk_documents_path = \"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_vector.json\"\n",
    "\n",
    "\n",
    "enriched_golden_evalset = enrich_eval_dataset_with_fully_optimized_rag_responses_vector_parallel(eval_dataset=golden_dataset, \n",
    "                                       chroma_path = chroma_path_fully_optimized_rag_pipeline_vector, \n",
    "                                       chunk_documents_path=chunk_documents_path,\n",
    "                                       model_name=\"gpt-4o-mini\",\n",
    "                                       max_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enriched_golden_evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d25d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"fully_optimized_rag_pipeline_vector_golden_qa_set\"\n",
    "\n",
    "generation_results_golden_dataset = run_generation_evaluation(json_filename=enriched_golden_evalset.split(\"/\")[-1], \n",
    "                                                              model_name=model_name, \n",
    "                                                              evaluation_mode=\"final_eval\"\n",
    "                                                              ) \n",
    "display(generation_results_golden_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57434b6",
   "metadata": {},
   "source": [
    "#### Manual Evaluation and LLM-as-a-Judge for Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run ../evaluation_datasets/generation_eval/manual_eval.py \"eval_datasets/golden_qa_evalset_generation_vector_rag_enriched.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"eval_datasets/golden_qa_evalset_generation_vector_rag_enriched.json\"\n",
    "first_output_path = \"eval_results/golden_qa_evalset_optimized_vector_rag_llm_as_a_judge_first_results.json\"\n",
    "final_rejudge_output_path = \"eval_results/golden_qa_evalset_optimized_vector_rag_llm_as_a_judge_final_rejudge_results.json\"\n",
    "max_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge for Comparison and Further Justification\n",
    "\n",
    "llm_as_a_judge_first_eval_results_path = run_llm_judge_parallel(input_path=input_path, output_path=first_output_path, max_workers=max_workers)\n",
    "llm_as_a_judge_rejudge_results_path = run_llm_rejudge_parallel(input_path=llm_as_a_judge_first_eval_results_path, output_path=final_rejudge_output_path, max_workers=max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3033ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_results_path = \"golden_qa_evalset_optimized_vector_rag_manual_results\"\n",
    "output_file_name_manual = \"1a_vector_manual_results\"\n",
    "output_file_name_LLMaaJ_first = \"1a_vector_llm_as_a_judge_first_results\"\n",
    "output_file_name_LLMaaJ_rejudge = \"1a_vector_llm_as_a_judge_rejudge_results\"\n",
    "\n",
    "eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(manual_results_path, output_file_name_manual)\n",
    "llm_as_a_judge_first_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_first_eval_results_path, output_file_name_LLMaaJ_first)\n",
    "llm_as_a_judge_final_rejudge_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_rejudge_results_path, output_file_name_LLMaaJ_rejudge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
