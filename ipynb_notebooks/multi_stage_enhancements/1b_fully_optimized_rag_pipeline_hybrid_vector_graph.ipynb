{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ffdabf",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation of Fully Optimized RAG - Hybrid (Vector + Graph) Approach\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Vector Database\n",
    "3. Querying the Vector Database\n",
    "4. Output of optimized RAG Pipelines\n",
    "5. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42010c7",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbcfe8",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Standard Library ===\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "from collections import Counter\n",
    "import threading\n",
    "import random\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm, trange\n",
    "from typing import List, Union, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "# === LangChain Core ===\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document  # (Optional: doppelt zu obigem)\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import TFIDFRetriever, EnsembleRetriever\n",
    "\n",
    "# === LangChain Community Integrationen ===\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# === OpenAI / LangChain OpenAI ===\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Lokale Projektmodule ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import (\n",
    "    clean_text,\n",
    "    save_documents_for_sparse_retrieval,\n",
    "    load_documents_for_sparse_retrieval,\n",
    "    save_to_chroma, \n",
    "    translate_query_to_german_if_needed,\n",
    "    load_vector_database,\n",
    "    generate_answer\n",
    ")\n",
    "\n",
    "from ipynb_notebooks.single_stage_enhancements.rankGPT_rerank import rankgpt_rerank\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import generate_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.llm_as_a_judge import run_llm_judge_parallel, run_llm_rejudge_parallel, calculate_and_visualize_scores_of_evaluation_scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d4023",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf22abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up one level from the Jupyter Notebook directory\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "# Construct the path to .env.neo4j in the base directory\n",
    "env_path = os.path.join(BASE_DIR, \".env.neo4j_aura\")\n",
    "\n",
    "# Load environment variables from .env and .env.neo4j files\n",
    "load_dotenv()\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Configure Neo4j Aura database connection\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma_dbs/chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137dc3c",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragraph_documents(datapath: str):\n",
    "    # Load JSON file\n",
    "    with open(datapath, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    documents = []\n",
    "    chunk_index = 1  # Track chunk index globally\n",
    "\n",
    "    for category in [\"laws\", \"ordinances\"]:\n",
    "        entries = data.get(category, [])\n",
    "        for entry in tqdm(entries, desc=f\"→ Processing {category}\"):\n",
    "            title = entry.get(\"title\", \"Unknown Title\")\n",
    "            paragraphs = entry.get(\"paragraphs\", [])\n",
    "\n",
    "            for para in tqdm(paragraphs, desc=f\"  ↳ Paragraphs in '{title}'\", leave=False):\n",
    "                para_url = para.get(\"paragraph_url\", \"\")\n",
    "                para_name = para.get(\"paragraph_name\", \"Unknown Paragraph\")\n",
    "\n",
    "                if para_url:\n",
    "                    try:\n",
    "                        # Load content from paragraph URL\n",
    "                        loader = WebBaseLoader(para_url)\n",
    "                        docs = loader.load()\n",
    "\n",
    "                        for doc in docs:\n",
    "                            raw_content = doc.page_content\n",
    "                            cleaned_content = clean_text(raw_content)\n",
    "                            doc.page_content = cleaned_content\n",
    "\n",
    "                            doc.metadata.update({\n",
    "                                \"law_title\": title,\n",
    "                                \"category\": category,\n",
    "                                \"paragraph_id\": para.get(\"paragraph_ID\"),\n",
    "                                \"paragraph_name\": para_name,\n",
    "                                \"paragraph_url\": para_url,\n",
    "                                \"chunk_id\": str(uuid.uuid4()),\n",
    "                                \"chunk_index\": chunk_index,\n",
    "                            })\n",
    "\n",
    "                            documents.append(doc)\n",
    "                            chunk_index += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading paragraph from URL {para_url}: {e}\")\n",
    "                else:\n",
    "                    print(f\"No paragraph URL found for {title}\")\n",
    "\n",
    "    if not documents:\n",
    "        raise ValueError(\"No paragraph documents could be loaded from the input.\")\n",
    "\n",
    "    print(f\"Successfully loaded {len(documents)} paragraph-level documents.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33ed07",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector Database with Paragraph-Wise Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARALLEL = 10\n",
    "SLEEP_BETWEEN_CALLS = 1\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def sanitize_relation(relation: str) -> str:\n",
    "    umlaut_map = {\"Ä\": \"AE\", \"Ö\": \"OE\", \"Ü\": \"UE\", \"ä\": \"ae\", \"ö\": \"oe\", \"ü\": \"ue\", \"ß\": \"ss\"}\n",
    "    for umlaut, replacement in umlaut_map.items():\n",
    "        relation = relation.replace(umlaut, replacement)\n",
    "    relation = relation.strip()\n",
    "    relation = relation.replace(\"§\", \"PARAGRAPH_\").replace(\" \", \"_\").replace(\"-\", \"_\").upper()\n",
    "    relation = re.sub(r\"[^A-Z0-9_]\", \"\", relation)\n",
    "    return relation or \"UNDEFINED_RELATION\"\n",
    "\n",
    "\n",
    "def extract_relations_from_chunk(text: str) -> list[dict]:\n",
    "    import openai  # wichtig: OpenAI-Client installiert & konfiguriert\n",
    "    system_prompt = (\n",
    "        \"Du bist ein KI-System für juristische Wissensmodellierung. \"\n",
    "        \"Extrahiere alle relevanten Entitäten und ihre Beziehungen aus folgendem Gesetzestext. \"\n",
    "        \"Gib das Ergebnis als reine JSON-Liste zurück:\\n\"\n",
    "        \"[{\\\"head\\\": \\\"...\\\", \\\"relation\\\": \\\"...\\\", \\\"tail\\\": \\\"...\\\"}]\"\n",
    "    )\n",
    "    user_prompt = f\"Text:\\n\\\"\\\"\\\"\\n{text}\\n\\\"\\\"\\\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content\n",
    "    try:\n",
    "        json_block = re.search(r\"\\[\\s*{.*?}\\s*\\]\", raw, re.DOTALL)\n",
    "        return json.loads(json_block.group()) if json_block else []\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing Error: {e}\\nAnswer: {raw}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "# Retry wrapper for write transactions with backoff in case of deadlocks\n",
    "def safe_write_transaction(driver, func, retries=3, *args, **kwargs):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with driver.session() as session:\n",
    "                session.write_transaction(func, *args, **kwargs)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if \"DeadlockDetected\" in str(e) and attempt < retries - 1:\n",
    "                print(f\"🔁 Retry due to deadlock (attempt {attempt + 1})\")\n",
    "                time.sleep(random.uniform(1.0, 2.0))  # backoff\n",
    "            else:\n",
    "                print(f\"❌ Write transaction failed: {e}\")\n",
    "                return\n",
    "\n",
    "# Processes a single chunk (document fragment)\n",
    "def process_single_chunk(i, doc, driver):\n",
    "    chunk_id = doc.metadata[\"chunk_id\"]\n",
    "    chunk_index = doc.metadata[\"chunk_index\"]\n",
    "    title = doc.metadata.get(\"title\", \"UnknownLaw\")\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    text = doc.page_content\n",
    "\n",
    "    # 1. Create or merge the Law node\n",
    "    def merge_law(tx):\n",
    "        tx.run(\"MERGE (l:Law {title: $title})\", {\"title\": title})\n",
    "    safe_write_transaction(driver, merge_law)\n",
    "\n",
    "    # 2. Create or update the Chunk node\n",
    "    def merge_chunk(tx):\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "            SET c.text = $text, c.chunk_index = $chunk_index, c.title = $title, c.source = $source\n",
    "        \"\"\", {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": text,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"title\": title,\n",
    "            \"source\": source\n",
    "        })\n",
    "    safe_write_transaction(driver, merge_chunk)\n",
    "\n",
    "    # 3. Create the HAS_CHUNK relationship\n",
    "    def link_law_chunk(tx):\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (l:Law {title: $title}), (c:Chunk {chunk_id: $chunk_id})\n",
    "            MERGE (l)-[:HAS_CHUNK]->(c)\n",
    "        \"\"\", {\"title\": title, \"chunk_id\": chunk_id})\n",
    "    safe_write_transaction(driver, link_law_chunk)\n",
    "\n",
    "    # 4. Extract relations from the chunk text and insert into graph\n",
    "    try:\n",
    "        relations = extract_relations_from_chunk(text)\n",
    "        for rel in relations:\n",
    "            head = rel[\"head\"]\n",
    "            tail = rel[\"tail\"]\n",
    "            rel_type = sanitize_relation(rel[\"relation\"])\n",
    "\n",
    "            def merge_relation(tx):\n",
    "                tx.run(f\"\"\"\n",
    "                    MERGE (h:Entity {{id: $head}})\n",
    "                    MERGE (t:Entity {{id: $tail}})\n",
    "                    MERGE (h)-[:{rel_type}]->(t)\n",
    "                    WITH h, t\n",
    "                    MATCH (c:Chunk {{chunk_id: $chunk_id}})\n",
    "                    MERGE (c)-[:HAS_ENTITY]->(h)\n",
    "                    MERGE (c)-[:HAS_ENTITY]->(t)\n",
    "                \"\"\", {\n",
    "                    \"head\": head,\n",
    "                    \"tail\": tail,\n",
    "                    \"chunk_id\": chunk_id\n",
    "                })\n",
    "\n",
    "            safe_write_transaction(driver, merge_relation)\n",
    "            time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in chunk {chunk_id[:6]}: {e}\")\n",
    "\n",
    "# Main ingest function\n",
    "def ingest_chunks_to_neo4j(chunks: list):\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_PARALLEL) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_single_chunk, i, doc, driver)\n",
    "            for i, doc in enumerate(chunks)\n",
    "        ]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Neo4j Ingest\"):\n",
    "            pass\n",
    "\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synchronized_databases(datapath, chunk_size: str = \"paragraph_wise_chunking\", chunk_overlap: str = \"no_overlap\", optimization: str = \"fully_optimized_rag_pipeline_hybrid\", baseline=False):\n",
    "\n",
    "    documents = load_paragraph_documents(datapath)\n",
    "    save_documents_for_sparse_retrieval(documents, chunk_size, chunk_overlap, optimization, baseline)\n",
    "\n",
    "    print(\"Storing in Chroma ...\")\n",
    "    chroma_path = save_to_chroma(documents, chunk_size, chunk_overlap, baseline, optimization)\n",
    "\n",
    "    print(\"Ingest in Neo4j ...\")\n",
    "    ingest_chunks_to_neo4j(documents)\n",
    "\n",
    "    print(\"Both databases were successfully synchronized.\")\n",
    "    return chroma_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138943e",
   "metadata": {},
   "source": [
    "### 3. Querying of Hybird Vector + Graph Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ce70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents(\n",
    "    results: Union[List[Document], List[Tuple[Document, float]]],\n",
    "    query: str,\n",
    "    score_threshold: float = 0.25\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filters documents based on relevance scores or cosine similarity using normalized scores.\n",
    "\n",
    "    Args:\n",
    "        results: List of Documents or (Document, Score) tuples.\n",
    "        query: The search query string.\n",
    "        score_threshold: Normalized similarity threshold between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        List of Documents that pass the similarity threshold.\n",
    "    \"\"\"\n",
    "    filtered_docs = []\n",
    "\n",
    "    # Case 1: Scores are already provided\n",
    "    if results and isinstance(results[0], tuple):\n",
    "        scores = [score for _, score in results]\n",
    "        min_score, max_score = min(scores), max(scores)\n",
    "\n",
    "        for doc, score in results:\n",
    "            # Normalize score\n",
    "            norm_score = (score - min_score) / (max_score - min_score + 1e-8)\n",
    "            if norm_score >= score_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "    else:\n",
    "        docs = results\n",
    "        doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "        embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        # Embed query and documents\n",
    "        query_vec = embedding_model.embed_query(text=query)\n",
    "        doc_vecs = embedding_model.embed_documents(texts=doc_texts)\n",
    "\n",
    "        # Compute similarity\n",
    "        similarity_scores = cosine_similarity([query_vec], doc_vecs)[0]\n",
    "\n",
    "        # Normalize\n",
    "        min_score, max_score = similarity_scores.min(), similarity_scores.max()\n",
    "\n",
    "        for doc, score in zip(docs, similarity_scores):\n",
    "            norm_score = (score - min_score) / (max_score - min_score + 1e-8)\n",
    "            if norm_score >= score_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "    return filtered_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10423b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query_text, vectordb, chunk_documents_path: str, k: int, rerank_k: int = 50, dense_percent: float = 0.5, thresh_hold: float = 0.25):\n",
    "    if len(vectordb) == 0:\n",
    "        return [], \"No documents available in the database.\"\n",
    "\n",
    "    query_de = translate_query_to_german_if_needed(query_text)\n",
    "    documents = []  \n",
    "    \n",
    "    # Hybrid-Retriever Method Dense + TF-IDF with EnsembleRetriever\n",
    "    documents = load_documents_for_sparse_retrieval(chunk_documents_path)\n",
    "\n",
    "    tf_idf_retriever = TFIDFRetriever.from_documents(documents)\n",
    "    tf_idf_retriever.k = rerank_k\n",
    "    sparse_retriever = tf_idf_retriever\n",
    "\n",
    "    dense_retriever = vectordb.as_retriever(search_kwargs={\"k\": rerank_k}, search_type=\"similarity\")\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[sparse_retriever, dense_retriever],\n",
    "        weights=[dense_percent, 1 - dense_percent]\n",
    "    )\n",
    "\n",
    "    result_documents = ensemble_retriever.get_relevant_documents(query_de)\n",
    "    result_documents = result_documents[:k]\n",
    "    \n",
    "    # Post-Retrieval Optimization: Filtering Documents upon Relevancy\n",
    "    filtered_documents = filter_documents(results=result_documents, query=query_text, score_threshold=thresh_hold)\n",
    "    \n",
    "    # Post-Retrieval Optimization: Reranking Retrieved Documents with RankGPT\n",
    "    reranked_documents = rankgpt_rerank(query_text, filtered_documents, model_name=\"gpt-4o-mini\", window_size=4, step=1)\n",
    "    \n",
    "    return reranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, last_response):\n",
    "    return f\"{original_query}. Hinweis: Beachte bei der Beantwortung auch: {last_response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44544cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity_with_embeddings(text1, text2, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between OpenAI embeddings of two texts.\n",
    "    \"\"\"\n",
    "    embeddings = openai.embeddings.create(\n",
    "        model=model,\n",
    "        input=[text1, text2]\n",
    "    )\n",
    "    \n",
    "    vec1 = np.array(embeddings.data[0].embedding)\n",
    "    vec2 = np.array(embeddings.data[1].embedding)\n",
    "    \n",
    "    return float(cosine_similarity([vec1], [vec2])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_consistent_answer(answers):\n",
    "    embeddings = embedding_model.embed_documents(texts=answers)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    avg_sim = similarity_matrix.mean(axis=1)\n",
    "    best_index = int(np.argmax(avg_sim))\n",
    "    return answers[best_index], avg_sim, similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eedb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_answer_similarity_heatmap(\n",
    "    answers: list[str],\n",
    "    labels: list[str] = None,\n",
    "    similarity_matrix: Optional[np.ndarray] = None,\n",
    "    avg_similarities: Optional[np.ndarray] = None,\n",
    "    title: str = \"LLM Answer Similarity\"\n",
    "):\n",
    "\n",
    "    if similarity_matrix is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = model.encode(answers)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Model {i+1}\" for i in range(len(answers))]\n",
    "\n",
    "    if avg_similarities is not None:\n",
    "        annotated_labels = [\n",
    "            f\"{name}\\nØ={avg:.4f}\" for name, avg in zip(labels, avg_similarities)\n",
    "        ]\n",
    "    else:\n",
    "        annotated_labels = labels\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    sns.heatmap(\n",
    "        similarity_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".4f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=annotated_labels,\n",
    "        square=True,\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"LLM Answer\")\n",
    "    plt.ylabel(\"LLM Answer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_query(chunk_id: str, driver):\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk {chunk_id: $chunk_id})-[:HAS_ENTITY]->(e1:Entity)\n",
    "    OPTIONAL MATCH (e1)-[r1]->(e2:Entity)\n",
    "    OPTIONAL MATCH (e2)-[r2]->(e3:Entity)\n",
    "    WITH c, \n",
    "         collect({head: e1.id, rel: type(r1), tail: e2.id}) +\n",
    "         collect({head: e2.id, rel: type(r2), tail: e3.id}) AS relations\n",
    "    UNWIND relations AS relmap\n",
    "    WITH c, relmap.head AS head, relmap.rel AS relation, relmap.tail AS tail\n",
    "    WHERE head IS NOT NULL AND relation IS NOT NULL AND tail IS NOT NULL\n",
    "    RETURN\n",
    "        c.chunk_id AS chunk_id,\n",
    "        c.chunk_index AS chunk_index,\n",
    "        c.title AS law_title,\n",
    "        head,\n",
    "        relation,\n",
    "        tail\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(query, {\"chunk_id\": chunk_id})\n",
    "            return [\n",
    "                {\n",
    "                    \"chunk_id\": record[\"chunk_id\"],\n",
    "                    \"chunk_index\": record[\"chunk_index\"],\n",
    "                    \"law_title\": record[\"law_title\"],\n",
    "                    \"context\": f'{record[\"head\"]} - {record[\"relation\"]} -> {record[\"tail\"]}'\n",
    "                }\n",
    "                for record in result\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in chunk {chunk_id[:6]}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_retriever_from_chunks(chunk_ids: List[str], top_k: int = 20) -> dict:\n",
    "\n",
    "    retrieved_nodes = []\n",
    "    \n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = [executor.submit(run_graph_query, cid, driver) for cid in chunk_ids]\n",
    "        for f in as_completed(futures):\n",
    "            retrieved_nodes.extend(f.result())\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    # Ranking nach Häufigkeit\n",
    "    index_counts = Counter(entry[\"chunk_index\"] for entry in retrieved_nodes)\n",
    "    top_indices = set(idx for idx, _ in index_counts.most_common(top_k))\n",
    "    filtered_nodes = [entry for entry in retrieved_nodes if entry[\"chunk_index\"] in top_indices]\n",
    "\n",
    "    # Tokenbegrenzung\n",
    "    contexts = list(dict.fromkeys(entry[\"context\"] for entry in filtered_nodes))\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    MAX_TOKENS = 3000\n",
    "    final_contexts = []\n",
    "    token_count = 0\n",
    "\n",
    "    for ctx in contexts:\n",
    "        tokens = len(enc.encode(ctx))\n",
    "        if token_count + tokens > MAX_TOKENS:\n",
    "            break\n",
    "        final_contexts.append(ctx)\n",
    "        token_count += tokens\n",
    "\n",
    "    chunk_indices = [entry[\"chunk_index\"] for entry in filtered_nodes]\n",
    "    law_titles = list(set(entry[\"law_title\"] for entry in filtered_nodes))\n",
    "\n",
    "    return {\n",
    "        \"prompt_context\": final_contexts,\n",
    "        \"retrieved_chunk_indices\": list(set(chunk_indices)),\n",
    "        \"retrieved_law_titles\": law_titles\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_optimized_rag_pipeline_hybrid(\n",
    "    query,\n",
    "    database,\n",
    "    chunk_documents_path: str,\n",
    "    k=5,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    n_consistency=5,\n",
    "    temperature=0.7,\n",
    "    heatmap=False\n",
    "):\n",
    "    retrieved_contexts = []\n",
    "    retrieved_sources = []\n",
    "    retrieved_ids = []\n",
    "    retrieved_indices = []\n",
    "    retrieved_ids_set = set()\n",
    "\n",
    "    # Vector retrieval\n",
    "    vector_results = retrieve_documents(\n",
    "        query_text=query,\n",
    "        vectordb=database,\n",
    "        chunk_documents_path=chunk_documents_path,\n",
    "        k=k\n",
    "    )\n",
    "\n",
    "    for doc in vector_results:\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\")\n",
    "        if chunk_id not in retrieved_ids_set:\n",
    "            retrieved_contexts.append(doc.page_content)\n",
    "            retrieved_sources.append(doc.metadata.get(\"source\"))\n",
    "            retrieved_ids.append(chunk_id)\n",
    "            retrieved_indices.append(doc.metadata.get(\"chunk_index\"))\n",
    "            retrieved_ids_set.add(chunk_id)\n",
    "            \n",
    "            \n",
    "    # Graph retrieval based on accumulated chunk IDs\n",
    "    graph_results = graph_retriever_from_chunks(chunk_ids=retrieved_ids, top_k=20)\n",
    "\n",
    "    # Combine both vector and graph contexts (remove duplicates)\n",
    "    vector_contexts = [doc.page_content for doc in vector_results]\n",
    "    graph_contexts = graph_results.get(\"prompt_context\", [])\n",
    "    combined_context = list(dict.fromkeys(graph_contexts + vector_contexts))\n",
    "\n",
    "    # Token-based context trimming\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    MAX_TOKENS = 10000\n",
    "    final_contexts = []\n",
    "    token_count = 0\n",
    "    for ctx in combined_context:\n",
    "        tokens = len(enc.encode(ctx))\n",
    "        if token_count + tokens > MAX_TOKENS:\n",
    "            break\n",
    "        final_contexts.append(ctx)\n",
    "        token_count += tokens\n",
    "\n",
    "    # Self-consistency generation\n",
    "    answers = []\n",
    "    for i in range(n_consistency):\n",
    "        try:\n",
    "            answer = generate_answer(final_contexts, query, model_name, temperature)\n",
    "            answers.append(answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at answer no. {i+1}: {e}\")\n",
    "            answers.append(\"\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    response, similarities, similarity_matrix = select_most_consistent_answer(answers)\n",
    "\n",
    "    if heatmap:\n",
    "        print(\"\\n--- Self-Consistency Answers ---\")\n",
    "        for i, a in enumerate(answers):\n",
    "            print(f\"[{i+1}] ({similarities[i]:.4f}): {a}\")\n",
    "        plot_answer_similarity_heatmap(\n",
    "            answers=answers,\n",
    "            labels=[f\"Ans {i+1}\" for i in range(n_consistency)],\n",
    "            similarity_matrix=similarity_matrix,\n",
    "            avg_similarities=similarities\n",
    "        )\n",
    "\n",
    "    return response.strip(), retrieved_sources, final_contexts, retrieved_ids, retrieved_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e16419",
   "metadata": {},
   "source": [
    "### 4. Output of Baseline RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57716be",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../data/laws_and_ordinances.json\"\n",
    "\n",
    "chroma_path_fully_optimized_rag_pipeline_hybrid = generate_synchronized_databases(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Welchen Anwendungsbereich umfasst §1 des Elektromobilitätsgesetz - EmoG?\"\n",
    "database = load_vector_database(chroma_path=chroma_path_fully_optimized_rag_pipeline_hybrid)\n",
    "chunk_documents_path=\"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_hybrid.json\"\n",
    "model_name = \"gpt-4o-mini\"  # or any other supported model\n",
    "\n",
    "response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = fully_optimized_rag_pipeline_hybrid(query=query, \n",
    "                                                                                                                                database=database,\n",
    "                                                                                                                                chunk_documents_path=chunk_documents_path,\n",
    "                                                                                                                                model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(f\"Query: {query} \\n\")\n",
    "print(f\"Response: {response} \\n\")\n",
    "print(f\"Sources: {sources} \\n\")\n",
    "print(f\"Retrieved Chunk Contexts: {retrieved_chunk_contexts} \\n\")\n",
    "print(f\"Retrieved Chunk Ids: {retrieved_chunk_ids} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c67ed3",
   "metadata": {},
   "source": [
    "### 4. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c8f45",
   "metadata": {},
   "source": [
    "#### Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = generate_evalset(chroma_db=chroma_path_fully_optimized_rag_pipeline_hybrid, test_set_size=50, \n",
    "                 query_distribution={\"single\": 0.6, \"multi_specific\": 0.2, \"multi_intra_document\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66161e0b",
   "metadata": {},
   "source": [
    "#### Enrich Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeeda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_fully_optimized_rag_responses_hybrid(eval_dataset, chroma_path, chunk_documents_path, model_name=\"gpt-4o-mini\"):\n",
    "    \n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "    \n",
    "    for entry in tqdm(eval_dataset_json, desc=\"Processing RAG responses\"):\n",
    "        query = entry[\"query\"]\n",
    "\n",
    "        # Run fully optimized RAG pipeline\n",
    "        response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = fully_optimized_rag_pipeline_hybrid(query=query, \n",
    "                                                                                                                                database=db,\n",
    "                                                                                                                                chunk_documents_path=chunk_documents_path,\n",
    "                                                                                                                                model_name=model_name)\n",
    "\n",
    "        # Add new fields to file\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "        enriched_dataset.append(entry)\n",
    "\n",
    "    output_path = f\"{eval_dataset.replace('.json', '')}_rag_enriched.json\"\n",
    "    # Store results as new json file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb00cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_documents_path = \"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_hybrid.json\"\n",
    "\n",
    "enriched_evalset = enrich_eval_dataset_with_fully_optimized_rag_responses_hybrid(eval_dataset=eval_dataset, \n",
    "                                       chroma_path = chroma_path_fully_optimized_rag_pipeline_hybrid, \n",
    "                                       chunk_documents_path=chunk_documents_path,\n",
    "                                       model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbae03",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11563415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"fully_optimized_rag_hybrid\"\n",
    "\n",
    "retrieval_result = run_retrieval_evaluation(json_filename=enriched_evalset.split(\"/\")[-1], model_name=model_name, evaluation_mode=\"final_eval\")\n",
    "display(retrieval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a807782",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56779465",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_results = run_generation_evaluation(json_filename=enriched_evalset.split(\"/\")[-1], model_name=model_name, evaluation_mode=\"final_eval\") \n",
    "display(generation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c867e",
   "metadata": {},
   "source": [
    "#### Evaluate RAG Generation on Golden Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ca3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chroma_path_fully_optimized_rag_pipeline_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f015e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = \"eval_datasets/golden_qa_evalset_generation.json\"\n",
    "chunk_documents_path = \"2960_documents_for_sparse_retrieval_paragraph_wise_chunking_no_overlap_fully_optimized_rag_pipeline_hybrid.json\"\n",
    "\n",
    "enriched_golden_evalset = enrich_eval_dataset_with_fully_optimized_rag_responses_hybrid(eval_dataset=golden_dataset, \n",
    "                                       chroma_path = chroma_path_fully_optimized_rag_pipeline_hybrid, \n",
    "                                       chunk_documents_path=chunk_documents_path,\n",
    "                                       model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_results_golden_dataset = run_generation_evaluation(json_filename=enriched_golden_evalset.split(\"/\")[-1], \n",
    "                                                              model_name=model_name, \n",
    "                                                              evaluation_mode=\"final_eval\") \n",
    "display(generation_results_golden_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c08663",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge for Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"eval_datasets/golden_qa_evalset_generation_hybrid_rag_enriched.json\"\n",
    "first_output_path = \"eval_results/golden_qa_evalset_optimized_hybrid_rag_llm_as_a_judge_first_results.json\"\n",
    "final_rejudge_output_path = \"eval_results/golden_qa_evalset_optimized_hybrid_rag_llm_as_a_judge_final_rejudge_results.json\"\n",
    "max_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97957a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge for Comparison and Further Justification\n",
    "\n",
    "llm_as_a_judge_first_eval_results_path = run_llm_judge_parallel(input_path=input_path, output_path=first_output_path, max_workers=max_workers)\n",
    "llm_as_a_judge_rejudge_results_path = run_llm_rejudge_parallel(input_path=llm_as_a_judge_first_eval_results_path, output_path=final_rejudge_output_path, max_workers=max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff071c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name_LLMaaJ_first = \"1b_hybrid_llm_as_a_judge_first_results\"\n",
    "output_file_name_LLMaaJ_rejudge = \"1b_hybrid_llm_as_a_judge_rejudge_results\"\n",
    "\n",
    "llm_as_a_judge_first_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_first_eval_results_path, output_file_name_LLMaaJ_first)\n",
    "llm_as_a_judge_final_rejudge_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_rejudge_results_path, output_file_name_LLMaaJ_rejudge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
