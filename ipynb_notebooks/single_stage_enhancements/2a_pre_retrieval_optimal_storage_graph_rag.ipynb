{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation of RAG with Knowledge Graph (GraphRAG)\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Graph Database\n",
    "3. Querying the Graph Database\n",
    "4. Output of Knowledge Graph RAG Model\n",
    "5. Evaluations\n",
    "\n",
    "This code is adapted and based on the provided implementation of Tomaz Bratanic [https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import logging\n",
    "import threading\n",
    "from uuid import uuid4\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tiktoken\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === LangChain Core Modules ===\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "# === LangChain Models and Tools ===\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# === LangChain Integrations ===\n",
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "# === Project-Specific Modules ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import load_documents\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_graph_dataset_generator import generate_graph_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up one level from the Jupyter Notebook directory\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "# Construct the path to .env.neo4j in the base directory\n",
    "env_path = os.path.join(BASE_DIR, \".env.neo4j\")\n",
    "\n",
    "# Load environment variables from .env and .env.neo4j files\n",
    "load_dotenv()\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Set environment variables for OpenAI and Neo4j\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "NEO4J_AUTH = os.getenv(\"NEO4J_AUTH\")\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "\n",
    "# Split NEO4J_AUTH into user name and password\n",
    "NEO4J_USERNAME, NEO4J_PASSWORD = NEO4J_AUTH.split(\"/\")\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation of Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_id(text: str) -> str:\n",
    "    \"\"\"Generates a stable ID based on the text content.\"\"\"\n",
    "    return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_chunk(text: str) -> list[dict]:\n",
    "    system_prompt = (\n",
    "        \"Du bist ein KI-System für juristische Wissensmodellierung. \"\n",
    "        \"Extrahiere alle relevanten Entitäten und ihre Beziehungen aus folgendem Gesetzestext. \"\n",
    "        \"Gib das Ergebnis als reine JSON-Liste zurück:\\n\"\n",
    "        \"[{\\\"head\\\": \\\"...\\\", \\\"relation\\\": \\\"...\\\", \\\"tail\\\": \\\"...\\\"}]\\n\"\n",
    "        \"Keine weiteren Erklärungen oder Einleitungen.\"\n",
    "    )\n",
    "    user_prompt = f\"Text:\\n\\\"\\\"\\\"\\n{text}\\n\\\"\\\"\\\"\\n\\nExtrahiere jetzt.\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        json_block = re.search(r\"\\[\\s*{.*?}\\s*\\]\", raw, re.DOTALL)\n",
    "        if not json_block:\n",
    "            raise ValueError(\"No JSON-Block found.\")\n",
    "        return json.loads(json_block.group())\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing Errpr: {e}\")\n",
    "        print(\"GPT-Antwort:\", raw)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_relation(relation: str) -> str:\n",
    "    umlaut_map = {\n",
    "        \"Ä\": \"AE\", \"Ö\": \"OE\", \"Ü\": \"UE\",\n",
    "        \"ä\": \"ae\", \"ö\": \"oe\", \"ü\": \"ue\",\n",
    "        \"ß\": \"ss\"\n",
    "    }\n",
    "\n",
    "    for umlaut, replacement in umlaut_map.items():\n",
    "        relation = relation.replace(umlaut, replacement)\n",
    "\n",
    "    relation = relation.strip()\n",
    "    relation = relation.replace(\"§\", \"PARAGRAPH_\")\n",
    "    relation = relation.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    relation = relation.upper()\n",
    "\n",
    "    # Remove everything excepts A-Z, 0-9 and _\n",
    "    relation = re.sub(r\"[^A-Z0-9_]\", \"\", relation)\n",
    "\n",
    "    if not relation:\n",
    "        relation = \"UNDEFINED_RELATION\"\n",
    "\n",
    "    return relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interlaw_references(graph: Neo4jGraph):\n",
    "    \"\"\"Scans all Entity nodes for legal cross-references and creates :CITES_LAW relationships between Law nodes.\"\"\"\n",
    "\n",
    "    print(\"Searching for inter-law references ...\")\n",
    "    \n",
    "    # Get all Entity nodes and their associated Law titles\n",
    "    result = graph.query(\"\"\"\n",
    "    MATCH (e:Entity)<-[:HAS_ENTITY]-(c:Chunk)<-[:HAS_CHUNK]-(l:Law)\n",
    "    RETURN e.id AS entity_id, l.title AS source_law\n",
    "    \"\"\")\n",
    "\n",
    "    references = []\n",
    "\n",
    "    for record in result:\n",
    "        entity_id = record[\"entity_id\"]\n",
    "        source_law = record[\"source_law\"]\n",
    "\n",
    "        # Look for references to other laws (e.g., \"§ 14 EnWG\", \"according to GEG\")\n",
    "        match = re.search(r\"(§\\s*\\d+[a-zA-Z]*(?: Abs\\.? \\d+)?(?: Satz \\d+)? )?\\b([A-ZÄÖÜ]{2,})\\b\", entity_id)\n",
    "        if match:\n",
    "            target_law = match.group(2)\n",
    "            # Avoid self-references\n",
    "            if target_law != source_law:\n",
    "                references.append((source_law, target_law))\n",
    "\n",
    "    # Deduplicate references\n",
    "    unique_refs = set(references)\n",
    "\n",
    "    # Create citation edges in the graph\n",
    "    for src, tgt in unique_refs:\n",
    "        graph.query(\"\"\"\n",
    "        MATCH (a:Law {title: $src}), (b:Law {title: $tgt})\n",
    "        MERGE (a)-[:CITES_LAW]->(b)\n",
    "        \"\"\", {\"src\": src, \"tgt\": tgt})\n",
    "    \n",
    "    print(f\"Created {len(unique_refs)} inter-law citation links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARALLEL = 10  # maximal gleichzeitig\n",
    "SLEEP_BETWEEN_CALLS = 2  # Sekunden, falls du auf Nummer sicher gehen willst\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_single_chunk(i, doc, graph):\n",
    "    chunk_text = doc.page_content\n",
    "    chunk_id = generate_chunk_id(chunk_text)\n",
    "    title = doc.metadata.get(\"title\", \"UnknownLaw\")\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "\n",
    "    doc.metadata[\"chunk_id\"] = chunk_id\n",
    "    doc.metadata[\"chunk_index\"] = i\n",
    "    doc.metadata[\"title\"] = title\n",
    "\n",
    "    # create law node and connect it with chunk nodes\n",
    "    with lock:\n",
    "        # create law node\n",
    "        graph.query(\"\"\"\n",
    "        MERGE (l:Law {title: $title})\n",
    "        \"\"\", {\"title\": title})\n",
    "\n",
    "        # create chunk node\n",
    "        graph.query(\"\"\"\n",
    "        MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "        SET c.text = $text, c.chunk_index = $chunk_index, c.title = $title, c.source = $source\n",
    "        \"\"\", {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text,\n",
    "            \"chunk_index\": i,\n",
    "            \"title\": title,\n",
    "            \"source\": source\n",
    "        })\n",
    "\n",
    "        # Connection between law and chunk\n",
    "        graph.query(\"\"\"\n",
    "        MATCH (l:Law {title: $title}), (c:Chunk {chunk_id: $chunk_id})\n",
    "        MERGE (l)-[:HAS_CHUNK]->(c)\n",
    "        \"\"\", {\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": chunk_id\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        relations = extract_relations_from_chunk(chunk_text)\n",
    "        for rel in relations:\n",
    "            head = rel[\"head\"]\n",
    "            tail = rel[\"tail\"]\n",
    "            relation = sanitize_relation(rel[\"relation\"])\n",
    "\n",
    "            cypher = f\"\"\"\n",
    "            MERGE (h:Entity {{id: $head}})\n",
    "            MERGE (t:Entity {{id: $tail}})\n",
    "            MERGE (h)-[:{relation}]->(t)\n",
    "            WITH h, t\n",
    "            MATCH (c:Chunk {{chunk_id: $chunk_id}})\n",
    "            MERGE (c)-[:HAS_ENTITY]->(h)\n",
    "            MERGE (c)-[:HAS_ENTITY]->(t)\n",
    "            \"\"\"\n",
    "            with lock:\n",
    "                graph.query(cypher, {\n",
    "                    \"head\": head,\n",
    "                    \"tail\": tail,\n",
    "                    \"chunk_id\": chunk_id\n",
    "                })\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at chunk {chunk_id[:6]}...: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_graph_under_chunks_parallel(datapath, chunk_size=1024, chunk_overlap=128):\n",
    "    raw_documents = load_documents(datapath)\n",
    "    splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_documents(raw_documents)\n",
    "\n",
    "    graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_PARALLEL) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_single_chunk, i, doc, graph)\n",
    "            for i, doc in enumerate(chunks)\n",
    "        ]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Parallel-Ingestion\"):\n",
    "            pass\n",
    "\n",
    "    graph.query(\"\"\"\n",
    "    MATCH (a:Chunk), (b:Chunk)\n",
    "    WHERE a.title = b.title AND a.chunk_id <> b.chunk_id\n",
    "    MERGE (a)-[:SAME_LAW]->(b)\n",
    "    \"\"\")\n",
    "    \n",
    "    create_interlaw_references(graph)\n",
    "    \n",
    "    print(\"Parallel-Ingestion succesful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to test with the sample text\n",
    "ingest_graph_under_chunks_parallel(datapath=\"../../data/laws_and_ordinances.json\", chunk_size=1024, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Querying of Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neo4j connection\n",
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Entity extraction schema\n",
    "class Entities(BaseModel):\n",
    "    names: List[str] = Field(..., description=\"Extracted legal entities such as laws, abbreviations, paragraph references, or authorities.\")\n",
    "\n",
    "# Define an output parser for structured extraction\n",
    "parser = PydanticOutputParser(pydantic_object=Entities)\n",
    "\n",
    "# Define a better prompt for structured entity extraction\n",
    "entity_prompt = PromptTemplate(\n",
    "    template=\"Extract legal entities (e.g. laws, abbreviations, paragraph references, institutions) from the following question:\\n\\n{question}\\n\\n{format_instructions}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting entities\n",
    "def extract_entities(question: str) -> Entities:\n",
    "    prompt_value = entity_prompt.format_prompt(question=question)\n",
    "    response = llm.invoke(prompt_value.to_string())  # Ensure correct format\n",
    "    return parser.parse(response.content)  # Ensure parsing of response\n",
    "\n",
    "# Create a LangChain tool\n",
    "entity_extraction_tool = Tool(\n",
    "    name=\"EntityExtractor\",\n",
    "    func=extract_entities,\n",
    "    description=\"Extracts named entities (people, organizations, locations) from a question.\"\n",
    ")\n",
    "\n",
    "# Chain with prompt\n",
    "entity_chain = entity_prompt | llm | parser\n",
    "\n",
    "# Full-text search query generation\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    input = input.lower().replace(\"§\", \"paragraph\").replace(\"-\", \" \").replace(\",\", \"\").strip()\n",
    "    return f'\"{input}\"~'\n",
    "\n",
    "\n",
    "# Create a full-text index if it doesn't exist\n",
    "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id, e.title, e.text]\")\n",
    "\n",
    "# Structured retrieval from Neo4j graph\n",
    "def structured_retriever(question: str, top_k: int = 50) -> dict:\n",
    "    \"\"\"Performs graph-based retrieval using legal entity and relation context.\"\"\"\n",
    "    extracted = entity_chain.invoke({\"question\": question})\n",
    "    entities = extracted.names\n",
    "\n",
    "    retrieved_nodes = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Cypher query mit Ego-Network approach from Hu et al. (2024)\n",
    "        response = graph.query(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE toLower(e.id) CONTAINS toLower($query)\n",
    "        CALL apoc.path.expand(e, '>', 'Entity', 1, 2) YIELD path\n",
    "        UNWIND relationships(path) AS r\n",
    "        WITH startNode(r) AS s, endNode(r) AS t, type(r) AS rel\n",
    "        OPTIONAL MATCH (s)<-[:HAS_ENTITY]-(c:Chunk)\n",
    "        RETURN DISTINCT\n",
    "            s.id AS head,\n",
    "            rel AS relation,\n",
    "            t.id AS tail,\n",
    "            c.chunk_id AS chunk_id,\n",
    "            c.chunk_index AS chunk_index,\n",
    "            c.title AS law_title\n",
    "        \"\"\", {\"query\": entity})\n",
    "\n",
    "        for row in response:\n",
    "            if not row[\"chunk_id\"]:\n",
    "                continue \n",
    "\n",
    "            retrieved_nodes.append({\n",
    "                \"chunk_id\": row[\"chunk_id\"],\n",
    "                \"chunk_index\": row[\"chunk_index\"],\n",
    "                \"law_title\": row[\"law_title\"],\n",
    "                \"context\": f'{row[\"head\"]} - {row[\"relation\"]} -> {row[\"tail\"]}'\n",
    "            })\n",
    "            \n",
    "            \n",
    "    # count context data\n",
    "    index_counts = Counter(entry[\"chunk_index\"] for entry in retrieved_nodes)\n",
    "        \n",
    "    # Get top-k most frequent chunk indices\n",
    "    top_indices = set(idx for idx, _ in index_counts.most_common(top_k))\n",
    "\n",
    "    # Filter to keep only nodes from top-k chunk indices\n",
    "    filtered_nodes = [entry for entry in retrieved_nodes if entry[\"chunk_index\"] in top_indices]\n",
    "        \n",
    "    # Filter: Only keep relations that contain named entities from the question --> Soft pruning by Hu et al. (2024)\n",
    "    contexts = list({\n",
    "        entry[\"context\"]\n",
    "        for entry in filtered_nodes\n",
    "        if any(ent.lower() in entry[\"context\"].lower() for ent in entities)\n",
    "    })\n",
    "    #contexts = list(set(entry[\"context\"] for entry in filtered_nodes))\n",
    "\n",
    "\n",
    "    \n",
    "    chunk_ids = [entry[\"chunk_id\"] for entry in filtered_nodes]\n",
    "    chunk_indices = [entry[\"chunk_index\"] for entry in filtered_nodes]\n",
    "    index_counts = dict(Counter(chunk_indices))\n",
    "    law_titles = list(set(entry[\"law_title\"] for entry in filtered_nodes))\n",
    "    \n",
    "    # Optional: truncate context to avoid token overflow (OpenAI token budget)\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    MAX_TOKENS = 3000\n",
    "\n",
    "    final_contexts = []\n",
    "    token_count = 0\n",
    "\n",
    "    for ctx in contexts:\n",
    "        tokens = len(enc.encode(ctx))\n",
    "        if token_count + tokens > MAX_TOKENS:\n",
    "            break\n",
    "        final_contexts.append(ctx)\n",
    "        token_count += tokens\n",
    "\n",
    "    return {\n",
    "        \"prompt_context\": final_contexts,\n",
    "        \"retrieved_chunk_ids\": list(dict.fromkeys(chunk_ids)),\n",
    "        \"retrieved_chunk_indices\": list(set(chunk_indices)),\n",
    "        \"retrieved_chunk_index_counts\": index_counts,\n",
    "        \"law_titles\": law_titles\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "def retriever(question: str):\n",
    "    \"\"\"\n",
    "    Retrieve structured context from the knowledge graph based on a legal question.\n",
    "    Processes entity-level graph relationships and returns relevant context information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the structured retrieval based on question → returns a dictionary\n",
    "    structured_data = structured_retriever(question)\n",
    "\n",
    "    # Extract fields from structured result dictionary\n",
    "    contexts = structured_data.get(\"prompt_context\", [])\n",
    "    chunk_ids = structured_data.get(\"retrieved_chunk_ids\", [])\n",
    "    chunk_indices = structured_data.get(\"retrieved_chunk_indices\", [])\n",
    "    chunk_index_counts = structured_data.get(\"retrieved_chunk_index_counts\", [])\n",
    "    law_titles = structured_data.get(\"law_titles\", [])\n",
    "\n",
    "    # Return final formatted output\n",
    "    return {\n",
    "        \"prompt_context\": contexts,\n",
    "        \"retrieved_chunk_ids\": chunk_ids,\n",
    "        \"retrieved_chunk_indices\": chunk_indices,\n",
    "        \"retrieved_chunk_index_counts\": chunk_index_counts,\n",
    "        \"retrieved_law_titles\": law_titles\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hard_prompt(contexts: List[str], law_titles: List[str]) -> str:\n",
    "    title_section = \"\\n\".join(f\"Gesetz: {t}\" for t in set(law_titles))\n",
    "    relation_section = \"\\n\".join(f\"- {c}\" for c in contexts)\n",
    "    return f\"{title_section}\\n\\nRelationen:\\n{relation_section}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Output of Baseline RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the main task (answering the question based on context)\n",
    "template = \"\"\"\n",
    "Du bist ein hilfreicher, juristischer KI-Assistent für Gesetzestexte im deutschen Energie- und Versorgungsbereich. \n",
    "Beantworte folgende Frage in der Sprache, in der sie gestellt wurde, und generiere eine kurze, präzise, konsistente und vollständige Antwort von max. 200 Tokens basierend auf folgenden kontextbasierten Relationen: \n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "graph_rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"retrieval\": RunnablePassthrough() | retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | (lambda inputs: {\n",
    "        **inputs,\n",
    "        \"context\": inputs[\"retrieval\"][\"prompt_context\"],\n",
    "        \"retrieved_chunk_ids\": inputs[\"retrieval\"][\"retrieved_chunk_ids\"],\n",
    "        \"retrieved_chunk_indices\": inputs[\"retrieval\"][\"retrieved_chunk_indices\"],\n",
    "        \"retrieved_chunk_index_counts\": inputs[\"retrieval\"][\"retrieved_chunk_index_counts\"],\n",
    "        \"law_titles\": list(set(inputs[\"retrieval\"].get(\"retrieved_law_titles\", []))) \n",
    "    })\n",
    "    | (lambda inputs: {\n",
    "        **inputs,\n",
    "        \"formatted_context\": format_hard_prompt(inputs[\"context\"], inputs.get(\"law_titles\", [\"Unbekanntes Gesetz\"]))\n",
    "    })\n",
    "    | (lambda inputs: {\n",
    "        **inputs,\n",
    "        \"prompt_text\": prompt.format(context=inputs[\"formatted_context\"], question=inputs[\"question\"])\n",
    "    })\n",
    "    | (lambda inputs: {\n",
    "        **inputs,\n",
    "        \"generated_response\": llm.invoke(inputs[\"prompt_text\"]).content\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph_rag_chain.invoke({\"question\": \"Welchen Anwendungsbereich umfasst §1 des Energiewirtschaftsgesetzes - EnWG?\"})\n",
    "\n",
    "print(\"Antwort:\")\n",
    "print(results[\"generated_response\"])\n",
    "print(\"Context:\")\n",
    "print(results[\"context\"])\n",
    "print(\"Chunk IDs:\")\n",
    "print(results[\"retrieved_chunk_ids\"])\n",
    "print(\"Chunk Indices:\")\n",
    "print(results[\"retrieved_chunk_indices\"])\n",
    "print(\"Chunk Counts:\")\n",
    "print(results[\"retrieved_chunk_index_counts\"])\n",
    "print(\"Law Titles:\")\n",
    "print(results[\"law_titles\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_eval_dataset_graph_rag = generate_graph_evalset(test_set_size=50, query_distribution={\"single\": 0.6, \"multi_specific\": 0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Enrich Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_graph_rag_chain(eval_dataset):\n",
    "    \"\"\"\n",
    "    Enriches a multi-hop evaluation dataset using a GraphRAG LangChain `Runnable`.\n",
    "    \"\"\"\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "\n",
    "    for entry in tqdm(eval_dataset_json, desc=\"Processing GraphRAG responses\"):\n",
    "        query = entry[\"query\"]\n",
    "\n",
    "        try:\n",
    "            results = graph_rag_chain.invoke({\"question\": query})\n",
    "\n",
    "            entry[\"generated_response\"] = results.get(\"generated_response\", \"\")\n",
    "            entry[\"retrieved_chunk_contexts\"] = results.get(\"context\", [])\n",
    "            entry[\"retrieved_chunk_ids\"] = results.get(\"retrieved_chunk_ids\", [])\n",
    "            entry[\"retrieved_chunk_indices\"] = results.get(\"retrieved_chunk_indices\", [])\n",
    "\n",
    "            enriched_dataset.append(entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"GraphRAG Error for query '{query}': {e}\")\n",
    "\n",
    "    output_path = f\"{eval_dataset.replace('json', '')}_enriched.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_graph_rag_enriched = enrich_eval_dataset_with_graph_rag_chain(eval_dataset=generated_eval_dataset_graph_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluation of Graph RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = f\"2_graph_database/{eval_dataset_graph_rag_enriched.split('/')[-1]}\"\n",
    "model_name=\"graph_rag\"\n",
    "\n",
    "retrieval_result = run_retrieval_evaluation(json_filename=json_filename, model_name=model_name)\n",
    "display(retrieval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Graph RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_results = run_generation_evaluation(json_filename=json_filename, model_name=model_name) \n",
    "display(generation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_last_line_from_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            return lines[-1].strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Beispielaufruf\n",
    "file_path = \"../../neo4j_db/import/graph_export.json\"\n",
    "last_line = read_last_line_from_json(file_path)\n",
    "\n",
    "if last_line:\n",
    "    print(\"📄 Letzte Zeile in der Datei:\")\n",
    "    print(last_line)\n",
    "else:\n",
    "    print(\"⚠️ Die Datei ist leer.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
