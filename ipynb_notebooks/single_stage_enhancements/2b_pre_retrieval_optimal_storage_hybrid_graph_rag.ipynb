{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation of RAG with Knowledge Graph in combination with Vector Database (Hybrid RAG)\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Graph & Vector Database\n",
    "3. Querying the Hybrid Database Combination\n",
    "4. Output of Hybrid  RAG Model\n",
    "5. Evaluations\n",
    "\n",
    "This code is adapted and based on the provided implementation of Tomaz Bratanic [https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, List\n",
    "from uuid import uuid4\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === LangChain Core Modules ===\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.tools import Tool\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# === LangChain Integrations ===\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "# === Neo4j Imports ===\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# === Project-Specific Module Imports ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import (\n",
    "    load_documents,\n",
    "    save_documents_for_sparse_retrieval,\n",
    "    load_vector_database,\n",
    "    retrieve_documents,\n",
    "    generate_answer\n",
    ")\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import (\n",
    "    generate_evalset\n",
    ")\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import (\n",
    "    run_retrieval_evaluation\n",
    ")\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import (\n",
    "    run_generation_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up one level from the Jupyter Notebook directory\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "# Construct the path to .env.neo4j in the base directory\n",
    "env_path = os.path.join(BASE_DIR, \".env.neo4j\")\n",
    "\n",
    "# Load environment variables from .env and .env.neo4j files\n",
    "load_dotenv()\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Set environment variables for OpenAI and Neo4j\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "NEO4J_AUTH = os.getenv(\"NEO4J_AUTH\")\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "\n",
    "\n",
    "# Split NEO4J_AUTH into user name and password\n",
    "NEO4J_USERNAME, NEO4J_PASSWORD = NEO4J_AUTH.split(\"/\")\n",
    "\n",
    "# Set up neo4j driver\n",
    "neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector and Graph Database with Identical Chunk Index\n",
    "\n",
    "**Why Creating a Chroma and Neo4j Database with Identical Chunk Index Is Necessary**\n",
    "\n",
    "Creating a Chroma and Neo4j database with identical chunk_id or chunk_index is necessary to enable hybrid retrieval in a RAG pipeline. This shared identifier ensures that semantically retrieved chunks from the vector database (Chroma) can be directly mapped to corresponding nodes and relationships in the graph database (Neo4j). It allows for seamless integration between semantic similarity and structured knowledge, supports graph-based context expansion, and makes the overall system traceable, explainable, and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARALLEL = 10\n",
    "SLEEP_BETWEEN_CALLS = 1\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def token_length(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document], chunk_size, chunk_overlap):\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    chunk_index = 1\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"chunk_id\"] = str(uuid.uuid4())\n",
    "        chunk.metadata[\"chunk_index\"] = chunk_index\n",
    "        chunk_index += 1\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document], chunk_size, chunk_overlap, optimization=\"hybrid_storage_approach\", batch_size=100):\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        print(f\"Removing existing directory: {CHROMA_PATH}\")\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    chroma_path = f\"../chroma_dbs/chroma_chunksize{chunk_size}_overlap{chunk_overlap}_{str(uuid.uuid4())[:8]}_{optimization}\"\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    db = Chroma(embedding_function=embeddings, persist_directory=chroma_path)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"ðŸ”¢ Store Chunks with Embeddings\"):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        db.add_documents(batch)\n",
    "\n",
    "    db.persist()\n",
    "    print(f\"âœ… Saved {len(chunks)} chunks to {chroma_path}\")\n",
    "    return chroma_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_relation(relation: str) -> str:\n",
    "    umlaut_map = {\"Ã„\": \"AE\", \"Ã–\": \"OE\", \"Ãœ\": \"UE\", \"Ã¤\": \"ae\", \"Ã¶\": \"oe\", \"Ã¼\": \"ue\", \"ÃŸ\": \"ss\"}\n",
    "    for umlaut, replacement in umlaut_map.items():\n",
    "        relation = relation.replace(umlaut, replacement)\n",
    "    relation = relation.strip()\n",
    "    relation = relation.replace(\"Â§\", \"PARAGRAPH_\").replace(\" \", \"_\").replace(\"-\", \"_\").upper()\n",
    "    relation = re.sub(r\"[^A-Z0-9_]\", \"\", relation)\n",
    "    return relation or \"UNDEFINED_RELATION\"\n",
    "\n",
    "\n",
    "def extract_relations_from_chunk(text: str) -> list[dict]:\n",
    "    import openai  # wichtig: OpenAI-Client installiert & konfiguriert\n",
    "    system_prompt = (\n",
    "        \"Du bist ein KI-System fÃ¼r juristische Wissensmodellierung. \"\n",
    "        \"Extrahiere alle relevanten EntitÃ¤ten und ihre Beziehungen aus folgendem Gesetzestext. \"\n",
    "        \"Gib das Ergebnis als reine JSON-Liste zurÃ¼ck:\\n\"\n",
    "        \"[{\\\"head\\\": \\\"...\\\", \\\"relation\\\": \\\"...\\\", \\\"tail\\\": \\\"...\\\"}]\"\n",
    "    )\n",
    "    user_prompt = f\"Text:\\n\\\"\\\"\\\"\\n{text}\\n\\\"\\\"\\\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content\n",
    "    try:\n",
    "        json_block = re.search(r\"\\[\\s*{.*?}\\s*\\]\", raw, re.DOTALL)\n",
    "        return json.loads(json_block.group()) if json_block else []\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing Error: {e}\\nAnswer: {raw}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_single_chunk(i, doc, graph):\n",
    "    chunk_id = doc.metadata[\"chunk_id\"]\n",
    "    chunk_index = doc.metadata[\"chunk_index\"]\n",
    "    title = doc.metadata.get(\"title\", \"UnknownLaw\")\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    text = doc.page_content\n",
    "\n",
    "    # Knoten erzeugen\n",
    "    graph.query(\"MERGE (l:Law {title: $title})\", {\"title\": title})\n",
    "    graph.query(\"\"\"\n",
    "        MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "        SET c.text = $text, c.chunk_index = $chunk_index, c.title = $title, c.source = $source\n",
    "    \"\"\", {\n",
    "        \"chunk_id\": chunk_id, \"text\": text, \"chunk_index\": chunk_index,\n",
    "        \"title\": title, \"source\": source\n",
    "    })\n",
    "    graph.query(\"\"\"\n",
    "        MATCH (l:Law {title: $title}), (c:Chunk {chunk_id: $chunk_id})\n",
    "        MERGE (l)-[:HAS_CHUNK]->(c)\n",
    "    \"\"\", {\n",
    "        \"title\": title, \"chunk_id\": chunk_id\n",
    "    })\n",
    "\n",
    "    # Relationen extrahieren\n",
    "    try:\n",
    "        relations = extract_relations_from_chunk(text)\n",
    "        for rel in relations:\n",
    "            head, tail = rel[\"head\"], rel[\"tail\"]\n",
    "            rel_type = sanitize_relation(rel[\"relation\"])\n",
    "            cypher = f\"\"\"\n",
    "                MERGE (h:Entity {{id: $head}})\n",
    "                MERGE (t:Entity {{id: $tail}})\n",
    "                MERGE (h)-[:{rel_type}]->(t)\n",
    "                WITH h, t\n",
    "                MATCH (c:Chunk {{chunk_id: $chunk_id}})\n",
    "                MERGE (c)-[:HAS_ENTITY]->(h)\n",
    "                MERGE (c)-[:HAS_ENTITY]->(t)\n",
    "            \"\"\"\n",
    "            graph.query(cypher, {\"head\": head, \"tail\": tail, \"chunk_id\": chunk_id})\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunk {chunk_id[:6]}: {e}\")\n",
    "\n",
    "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "\n",
    "def ingest_chunks_to_neo4j(chunks: list[Document]):\n",
    "    from langchain_community.graphs import Neo4jGraph\n",
    "    graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_PARALLEL) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_single_chunk, i, doc, graph)\n",
    "            for i, doc in enumerate(chunks)\n",
    "        ]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Neo4j Ingest\"):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synchronized_databases(datapath, chunk_size=512, chunk_overlap=64, optimization=\"hybrid_storage_approach\", baseline=False):\n",
    "\n",
    "    documents = load_documents(datapath)\n",
    "    chunks = split_text(documents, chunk_size, chunk_overlap)\n",
    "    save_documents_for_sparse_retrieval(chunks, chunk_size, chunk_overlap, optimization, baseline)\n",
    "\n",
    "    print(\"Storing in Chroma ...\")\n",
    "    chroma_path = save_to_chroma(chunks, chunk_size, chunk_overlap, optimization)\n",
    "\n",
    "    print(\"Ingest in Neo4j ...\")\n",
    "    ingest_chunks_to_neo4j(chunks)\n",
    "\n",
    "    print(\"Both databases were successfully synchronized.\")\n",
    "    return chroma_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path_hybrid_graph_rag = \"../chroma_dbs/chroma_chunksize1024_overlap128_a5e9b634_hybrid_graph_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../data/laws_and_ordinances.json\"\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 128\n",
    "optimization = \"hybrid_graph_rag\"\n",
    "\n",
    "chroma_path_hybrid_graph_rag = generate_synchronized_databases(datapath, chunk_size, chunk_overlap, optimization=optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Querying of Vector and Graph Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_query(chunk_id: str):\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk {chunk_id: $chunk_id})-[:HAS_ENTITY]->(e:Entity)\n",
    "    CALL apoc.path.expand(e, '>', 'Entity', 1, 2) YIELD path\n",
    "    UNWIND relationships(path) AS r\n",
    "    WITH c, startNode(r) AS s, endNode(r) AS t, type(r) AS rel\n",
    "    RETURN\n",
    "        c.chunk_id AS chunk_id,\n",
    "        s.id AS head,\n",
    "        rel AS relation,\n",
    "        t.id AS tail,\n",
    "        c.chunk_index AS chunk_index,\n",
    "        c.title AS law_title\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with neo4j_driver.session() as session:\n",
    "            result = session.run(query, {\"chunk_id\": chunk_id})\n",
    "            return [\n",
    "                {\n",
    "                    \"chunk_id\": record[\"chunk_id\"],\n",
    "                    \"chunk_index\": record[\"chunk_index\"],\n",
    "                    \"law_title\": record[\"law_title\"],\n",
    "                    \"context\": f'{record[\"head\"]} - {record[\"relation\"]} -> {record[\"tail\"]}'\n",
    "                }\n",
    "                for record in result\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error for chunk {chunk_id[:6]}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_retriever_from_chunks(chunk_ids: List[str], top_k: int = 20) -> dict:\n",
    "    retrieved_nodes = []\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = [executor.submit(run_graph_query, cid) for cid in chunk_ids]\n",
    "        for f in as_completed(futures):\n",
    "            retrieved_nodes.extend(f.result())\n",
    "\n",
    "    # Der Rest bleibt gleich wie vorher:\n",
    "    index_counts = Counter(entry[\"chunk_index\"] for entry in retrieved_nodes)\n",
    "    top_indices = set(idx for idx, _ in index_counts.most_common(top_k))\n",
    "    filtered_nodes = [entry for entry in retrieved_nodes if entry[\"chunk_index\"] in top_indices]\n",
    "\n",
    "    contexts = list(dict.fromkeys(entry[\"context\"] for entry in filtered_nodes))\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    MAX_TOKENS = 3000\n",
    "    final_contexts = []\n",
    "    token_count = 0\n",
    "\n",
    "    for ctx in contexts:\n",
    "        tokens = len(enc.encode(ctx))\n",
    "        if token_count + tokens > MAX_TOKENS:\n",
    "            break\n",
    "        final_contexts.append(ctx)\n",
    "        token_count += tokens\n",
    "\n",
    "    chunk_indices = [entry[\"chunk_index\"] for entry in filtered_nodes]\n",
    "    law_titles = list(set(entry[\"law_title\"] for entry in filtered_nodes))\n",
    "\n",
    "    return {\n",
    "        \"prompt_context\": final_contexts,\n",
    "        \"retrieved_chunk_indices\": list(set(chunk_indices)),\n",
    "        \"retrieved_law_titles\": law_titles\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_graph_rag_pipeline(query, database, model_name=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Hybrid RAG pipeline: First, a vector-based retrieval phase for chunk selection,     \n",
    "    followed by graph-based context enrichment via the corresponding chunk IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vektor-Retrieval\n",
    "    vector_results = retrieve_documents(query, db=database)\n",
    "    \n",
    "    if vector_results and isinstance(vector_results[0], tuple):\n",
    "        vector_results = [doc for doc, _ in vector_results]\n",
    "\n",
    "    # extract relevant Chunk-IDs\n",
    "    sources = [doc.metadata.get(\"source\") for doc in vector_results]\n",
    "    retrieved_chunk_ids = [doc.metadata.get(\"chunk_id\") for doc in vector_results]\n",
    "    retrieved_chunk_indices = [doc.metadata.get(\"chunk_index\") for doc in vector_results]\n",
    "\n",
    "    # extract graph results\n",
    "    graph_results = graph_retriever_from_chunks(retrieved_chunk_ids)\n",
    "\n",
    "    # extract vector and graph contexts \n",
    "    vector_contexts = [doc.page_content for doc in vector_results]\n",
    "    graph_contexts = graph_results.get(\"prompt_context\", [])\n",
    "\n",
    "    # combine and merge vector and graph contexts\n",
    "    merged_context = list(dict.fromkeys(graph_contexts + vector_contexts))\n",
    "\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    MAX_TOKENS = 10000\n",
    "    final_contexts = []\n",
    "    token_count = 0\n",
    "    for ctx in merged_context:\n",
    "        tokens = len(enc.encode(ctx))\n",
    "        if token_count + tokens > MAX_TOKENS:\n",
    "            break\n",
    "        final_contexts.append(ctx)\n",
    "        token_count += tokens\n",
    "        \n",
    "    # generate answer\n",
    "    response = generate_answer(final_contexts, query, model_name)\n",
    "\n",
    "    return response, sources, final_contexts, retrieved_chunk_ids, retrieved_chunk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Output of Hybrid RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Welchen Anwendungsbereich umfasst Â§1 des ElektromobilitÃ¤tsgesetz - EmoG?\"\n",
    "database = load_vector_database(chroma_path=chroma_path_hybrid_graph_rag)\n",
    "model_name = \"gpt-4o-mini\"  # or any other supported model\n",
    "\n",
    "response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = hybrid_graph_rag_pipeline(query=query, database=database, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(f\"Query: {query} \\n\")\n",
    "print(f\"Response: {response} \\n\")\n",
    "print(f\"Sources: {sources} \\n\")\n",
    "print(f\"Retrieved Chunk Contexts: {retrieved_chunk_contexts} \\n\")\n",
    "print(f\"Retrieved Chunk Indices: {retrieved_chunk_indices} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_hybrid_graph_rag = generate_evalset(chroma_db=chroma_path_hybrid_graph_rag, test_set_size=50, \n",
    "                 query_distribution={\"single\": 0.6, \"multi_specific\": 0.2, \"multi_intra_document\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Enrich Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_hybrid_graph_rag_responses(eval_dataset, chroma_path, model_name=\"gpt-4o-mini\"):\n",
    "    \n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "    \n",
    "    for entry in tqdm(eval_dataset_json, desc=\"Processing Hybrid RAG responses\"):\n",
    "        query = entry[\"query\"]\n",
    "\n",
    "        # Run RAG pipeline\n",
    "        response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = hybrid_graph_rag_pipeline(query, db, model_name=model_name)\n",
    "\n",
    "        # Add new fields to file\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "        enriched_dataset.append(entry)\n",
    "\n",
    "    output_path = f\"{eval_dataset.replace('.json', '')}_rag_enriched.json\"\n",
    "    # Store results as new json file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_graph_rag_enriched = enrich_eval_dataset_with_hybrid_graph_rag_responses(eval_dataset=eval_dataset_hybrid_graph_rag,\n",
    "                                                                                      chroma_path=chroma_path_hybrid_graph_rag,\n",
    "                                                                                      model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluation of Graph RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = f\"2_graph_database/{eval_dataset_graph_rag_enriched.split('/')[-1]}\"\n",
    "model_name=\"graph_rag\"\n",
    "\n",
    "retrieval_result = run_retrieval_evaluation(json_filename=json_filename, model_name=model_name)\n",
    "display(retrieval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Graph RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_results = run_generation_evaluation(json_filename=json_filename, model_name=model_name) \n",
    "display(generation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
