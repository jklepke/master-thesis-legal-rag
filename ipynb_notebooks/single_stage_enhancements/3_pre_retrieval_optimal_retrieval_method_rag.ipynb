{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aab8476",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation for Optimal Retriever in RAG\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Vector Database\n",
    "3. Querying the Vector Database\n",
    "4. Output of RAG Experiments\n",
    "5. Evaluations\n",
    "\n",
    "This notebook uses functions from the Baseline RAG .ipynb file and adapts these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee75dc",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a150b19",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe95eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === OpenAI Integration ===\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# === LangChain Core Modules ===\n",
    "from langchain.schema import Document\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import BM25Retriever, TFIDFRetriever, EnsembleRetriever\n",
    "\n",
    "# === Type Hints ===\n",
    "from typing import Any, List\n",
    "\n",
    "# === Project Root Configuration ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# === Local Project Modules ===\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import (\n",
    "    load_documents_for_sparse_retrieval,\n",
    "    load_vector_database,\n",
    "    generate_answer,\n",
    "    translate_query_to_german_if_needed\n",
    ")\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import generate_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349942b",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57123835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables. Assumes that the project directory contains a .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma_dbs/chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bbaee",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector Databases\n",
    "\n",
    "**Why Creating Separate Chroma Databases for Each Retrieval Strategy Is Not Necessary**\n",
    "\n",
    "In contrast to chunking experiments, evaluating different retrieval strategies does not require generating separate Chroma vector databases. This is because all strategies operate over the same underlying document corpus and embeddings. Retrieval techniques such as Dense Similarity Search, BM25, TF-IDF, Multi-Query, or Hybrid approaches differ only in how they search or rank the embedded documents—not in how the documents are chunked or stored.\n",
    "\n",
    "As long as the Chroma DB is generated using a consistent chunking strategy and embedding model, it provides a shared semantic space that is sufficient for fair comparison across retrieval methods. Creating separate vector stores per strategy would introduce unnecessary redundancy and would not improve the validity of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_optimal_retrieval_method = \"../chroma_dbs/chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_method\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5f92c",
   "metadata": {},
   "source": [
    "### 3. Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ad262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_query(base_prompt: str, n_variants: int) -> List[str]:\n",
    "    model = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    # Geänderter System- und Human-Prompt\n",
    "    system_msg = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an assistant that rewrites search phrases into alternative forms to broaden retrieval coverage.\"\n",
    "    )\n",
    "    human_msg = HumanMessagePromptTemplate.from_template(\n",
    "        \"Please create {n_variants} alternative search queries based on the following input: {base_prompt}. \"\n",
    "        \"List each query on a new line without numbering or bullet points.\"\n",
    "    )\n",
    "\n",
    "    prompt_chain = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "    formatted_messages = prompt_chain.format_prompt(base_prompt=base_prompt, n_variants=n_variants).to_messages()\n",
    "    \n",
    "    final_input = formatted_messages[0].content + \"\\n\" + formatted_messages[1].content\n",
    "    response = model(final_input)\n",
    "    \n",
    "    query_set = [base_prompt]\n",
    "    query_set += response.strip().split(\"\\n\")\n",
    "\n",
    "    return query_set\n",
    "\n",
    "def create_multi_query_list(prompt_batch: List[str], n_variants: int, output_name: str) -> List[List[str]]:\n",
    "    collected_queries = []\n",
    "\n",
    "    for idx, original_prompt in enumerate(tqdm(prompt_batch, desc=\"Generating query variants\")):\n",
    "        variants = create_multi_query(original_prompt, n_variants)\n",
    "        collected_queries.append({\n",
    "            \"prompt_id\": idx + 1,\n",
    "            \"query_variants\": variants\n",
    "        })\n",
    "\n",
    "    output_file = f\"../retrieval_inputs/multi_queries/{output_name}.json\"\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(collected_queries, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return [entry[\"query_variants\"] for entry in collected_queries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abfe04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_hypo_docs(query: str) -> Document:\n",
    "    llm_model = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"You are a specialist in German energy legislation. Formulate a concise explanatory paragraph (3–4 sentences) that addresses the legal question below in a clear and expert tone.\"\"\"\n",
    "    )\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"Legal Query: {question}\"\n",
    "    )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "    formatted_input = prompt_template.format_prompt(question=query).to_messages()\n",
    "    full_prompt = formatted_input[0].content + \"\\n\" + formatted_input[1].content\n",
    "\n",
    "    generated_response = llm_model(full_prompt)\n",
    "\n",
    "    doc = Document(page_content=generated_response, metadata={\"original_query\": query})\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def create_document_hypo_docs_list(query_list: List[str], path_to_save: str) -> List[Document]:\n",
    "    documents_generated: List[Document] = []\n",
    "    export_data = []\n",
    "\n",
    "    for idx, q in enumerate(tqdm(query_list, desc=\"Generating hypothetical documents\")):\n",
    "        doc = create_document_hypo_docs(q)\n",
    "        documents_generated.append(doc)\n",
    "\n",
    "        export_data.append({\n",
    "            \"query_id\": idx + 1,\n",
    "            \"hypo_docs_doc\": {\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "        })\n",
    "\n",
    "    output_path = f\"../retrieval_inputs/hypo_documents/{path_to_save}.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(export_data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return documents_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dense(vector_db, query, top_k):\n",
    "    \n",
    "    return [doc for doc, _ in vector_db.similarity_search_with_relevance_scores(query, k=top_k)]\n",
    "\n",
    "\n",
    "def retrieve_sparse(doc_source_path, query, top_k, backend=\"BM25\"):\n",
    "    \n",
    "    chunked_docs = load_documents_for_sparse_retrieval(doc_source_path)\n",
    "    retriever_cls = BM25Retriever if backend == \"BM25\" else TFIDFRetriever\n",
    "    retriever = retriever_cls.from_documents(chunked_docs)\n",
    "    retriever.k = top_k\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def retrieve_hybrid_mmr(doc_source_path, vector_db, user_input, final_k, backend=\"BM25\"):\n",
    "    \n",
    "    sparse_docs = load_documents_for_sparse_retrieval(doc_source_path)\n",
    "    mmr_dense = vector_db.max_marginal_relevance_search(query=user_input, k=25, fetch_k=50)\n",
    "\n",
    "    sparse_cls = BM25Retriever if backend == \"BM25\" else TFIDFRetriever\n",
    "    sparse = sparse_cls.from_documents(sparse_docs)\n",
    "    sparse.k = 25\n",
    "    bm_result = sparse.get_relevant_documents(user_input)\n",
    "\n",
    "    return EnsembleRetriever(retrievers=[vector_db.as_retriever(), sparse]).weighted_reciprocal_rank(\n",
    "        [mmr_dense, bm_result]\n",
    "    )[:final_k]\n",
    "\n",
    "\n",
    "def retrieve_reranked_hybrid(doc_source_path, vector_db, query, rerank_limit, final_k, backend=\"BM25\", dense_weight=0.5):\n",
    "    \n",
    "    raw_docs = load_documents_for_sparse_retrieval(doc_source_path)\n",
    "    sparse_cls = BM25Retriever if backend == \"BM25\" else TFIDFRetriever\n",
    "    sparse_ret = sparse_cls.from_documents(raw_docs)\n",
    "    sparse_ret.k = rerank_limit\n",
    "\n",
    "    dense_ret = vector_db.as_retriever(search_kwargs={\"k\": rerank_limit}, search_type=\"similarity\")\n",
    "    hybrid = EnsembleRetriever(retrievers=[sparse_ret, dense_ret], weights=[dense_weight, 1 - dense_weight])\n",
    "    return hybrid.get_relevant_documents(query)[:final_k]\n",
    "\n",
    "\n",
    "def retrieve_multi_query(vector_db, user_input, top_k, sub_queries):\n",
    "    \n",
    "    sub_queries = sub_queries or create_multi_query(user_input, 3)\n",
    "    retriever_list = [vector_db.as_retriever(search_kwargs={\"k\": top_k}) for _ in sub_queries]\n",
    "    results_all = [r.get_relevant_documents(q) for r, q in zip(retriever_list, sub_queries)]\n",
    "    return EnsembleRetriever(retrievers=retriever_list).weighted_reciprocal_rank(results_all)[:top_k]\n",
    "\n",
    "\n",
    "def retrieve_hybrid_multi_query(doc_source_path, vector_db, user_input, top_k, sub_queries):\n",
    "    \n",
    "    sparse_docs = load_documents_for_sparse_retrieval(doc_source_path)\n",
    "    query_variants = sub_queries or create_multi_query(user_input, 3)\n",
    "    result_pool = []\n",
    "    retriever_pool = []\n",
    "\n",
    "    for q in query_variants:\n",
    "        dense_ret = vector_db.as_retriever(search_kwargs={\"k\": top_k})\n",
    "        retriever_pool.append(dense_ret)\n",
    "        result_pool.append(dense_ret.get_relevant_documents(q))\n",
    "\n",
    "        bm25_ret = BM25Retriever.from_documents(sparse_docs)\n",
    "        retriever_pool.append(bm25_ret)\n",
    "        bm25_ret.k = top_k\n",
    "        result_pool.append(bm25_ret.get_relevant_documents(q))\n",
    "\n",
    "    return EnsembleRetriever(retrievers=retriever_pool).weighted_reciprocal_rank(result_pool)[:top_k]\n",
    "\n",
    "\n",
    "def retrieve_hypo_docs(vector_db, hypo_docs_input_doc, user_input, top_k):\n",
    "    \n",
    "    if not hypo_docs_input_doc:\n",
    "        hypo_docs_input_doc = create_document_hypo_docs(user_input)\n",
    "    return vector_db.similarity_search(hypo_docs_input_doc.page_content, k=top_k)\n",
    "\n",
    "\n",
    "def retrieve_hybrid_hypo_docs(doc_source_path, vector_db, hypo_docs_input_doc, user_input, top_k):\n",
    "    \n",
    "    if not hypo_docs_input_doc:\n",
    "        hypo_docs_input_doc = create_document_hypo_docs(user_input)\n",
    "    all_docs = load_documents_for_sparse_retrieval(doc_source_path)\n",
    "\n",
    "    sparse = BM25Retriever.from_documents(all_docs)\n",
    "    sparse.k = top_k\n",
    "    dense = vector_db.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "    hybrid = EnsembleRetriever(retrievers=[sparse, dense], weights=[0.5, 0.5])\n",
    "    return hybrid.get_relevant_documents(hypo_docs_input_doc.page_content)[:top_k]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(vector_db, chunk_documents_path: str, strategy: str, top_k: int, user_query: str,\n",
    "                       score_cutoff: int = 0.75, rerank_limit: int = 50, dense_ratio: float = 0.5,\n",
    "                       hypo_doc: Document = None, multi_queries: List[str] = []):\n",
    "\n",
    "    translated_query = translate_query_to_german_if_needed(user_query)\n",
    "\n",
    "    retrieval_dispatcher = { \n",
    "                            \"TF-IDF\": lambda: retrieve_sparse(doc_source_path=chunk_documents_path, query=translated_query, top_k=top_k, backend=\"TF-IDF\"), \n",
    "                            \"BM25\": lambda: retrieve_sparse(doc_source_path=chunk_documents_path, query=translated_query, top_k=top_k, backend=\"BM25\"), \n",
    "                            \"Dense\": lambda: retrieve_dense(vector_db, query=translated_query, top_k=top_k), \n",
    "                            \"MMR\": lambda: vector_db.max_marginal_relevance_search(query=user_query, k=top_k, fetch_k=50),                             \n",
    "                            \"Hypo_Docs\": lambda: retrieve_hypo_docs(vector_db=vector_db, hypo_docs_input_doc=hypo_doc, user_input=user_query, top_k=top_k), \n",
    "                            \"Multiple_Queries\": lambda: retrieve_multi_query(vector_db=vector_db, user_input=user_query, top_k=top_k, sub_queries=multi_queries),                            \n",
    "                            \"Hybrid_BM25_Dense\": lambda: retrieve_reranked_hybrid(doc_source_path=chunk_documents_path, vector_db=vector_db, query=translated_query, rerank_limit=rerank_limit, final_k=top_k, backend=\"BM25\", dense_weight=dense_ratio), \n",
    "                            \"Hybrid_TF-IDF_Dense\": lambda: retrieve_reranked_hybrid(doc_source_path=chunk_documents_path, vector_db=vector_db, query=translated_query, rerank_limit=rerank_limit, final_k=top_k, backend=\"TF-IDF\", dense_weight=dense_ratio), \n",
    "                            \"Hybrid_MMR_BM25\": lambda: retrieve_hybrid_mmr(doc_source_path=chunk_documents_path, vector_db=vector_db, user_input=user_query, final_k=top_k, backend=\"BM25\"),\n",
    "                            \"Hybrid_Hypo_Docs\": lambda: retrieve_hybrid_hypo_docs(doc_source_path=chunk_documents_path, vector_db=vector_db, hypo_docs_input_doc=hypo_doc, user_input=user_query, top_k=top_k), \n",
    "                            \"Hybrid_Multiple_Queries\": lambda: retrieve_hybrid_multi_query(doc_source_path=chunk_documents_path, vector_db=vector_db, user_input=user_query, top_k=top_k, sub_queries=multi_queries), \n",
    "                            }\n",
    "\n",
    "    if strategy not in retrieval_dispatcher:\n",
    "        raise ValueError(f\"Retrieval strategy '{strategy}' not supported.\")\n",
    "\n",
    "    return retrieval_dispatcher[strategy]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_retrieval_method(query, database, chunk_documents_path, k, model_name:str=\"gpt-4o-mini\", retrieval_method: str=\"Dense\", hypo_document: Document=None, multiple_queries: List[str]=[]):\n",
    "    \n",
    "    results = retrieve_documents(vector_db=database, chunk_documents_path=chunk_documents_path, strategy=retrieval_method, top_k=k, \n",
    "                                 user_query=query, hypo_doc=hypo_document, multi_queries=multiple_queries)\n",
    "\n",
    "    response = generate_answer(results, query, model_name)\n",
    "    \n",
    "    sources = [doc.metadata.get(\"source\") for doc in results]\n",
    "    retrieved_chunk_contexts = [doc.page_content for doc in results]\n",
    "    retrieved_chunk_ids = [doc.metadata.get(\"chunk_id\") for doc in results]\n",
    "    retrieved_chunk_indices = [doc.metadata.get(\"chunk_index\") for doc in results]\n",
    "\n",
    "    return response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_rag_responses_for_optimal_retrieval(eval_dataset, chroma_path, chunk_documents_path, k, model_name, retrieval_method, hypo_document, multiple_queries, optimization=\"3_optimal_retrieval_method/\"):\n",
    "    \n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "    \n",
    "    for i, entry in enumerate(tqdm(eval_dataset_json, desc=\"Processing RAG responses\")):\n",
    "        query = entry[\"query\"]\n",
    "        \n",
    "        hypo_doc = hypo_document[i] if retrieval_method in [\"Hypo_Docs\", \"Hybrid_Hypo_Docs\"] and hypo_document else None\n",
    "        multi_query = multiple_queries[i] if retrieval_method in [\"Multi_Query\", \"Hybrid_Multi_Query\"] and multiple_queries else None\n",
    "\n",
    "        response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline_retrieval_method(query=query, \n",
    "                                                                                                                              database=db, \n",
    "                                                                                                                              chunk_documents_path=chunk_documents_path, \n",
    "                                                                                                                              k=k, \n",
    "                                                                                                                              model_name=model_name, \n",
    "                                                                                                                              retrieval_method=retrieval_method,\n",
    "                                                                                                                              hypo_document=hypo_doc,\n",
    "                                                                                                                              multiple_queries=multi_query)\n",
    "\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "        enriched_dataset.append(entry)\n",
    "\n",
    "    output_path = f\"eval_datasets/{optimization}{eval_dataset.split('/')[-1].replace('.json', '')}_{retrieval_method}.json\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6e47c",
   "metadata": {},
   "source": [
    "### 4. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e51dc",
   "metadata": {},
   "source": [
    "#### 4.1 Preparing the Evaluation Dataset and Documents for Multi Query and Hypothetical Documents\n",
    "\n",
    "Since no new Chroma DB had to be created, the evaluation data set from the RAG baseline can also be reused. The data set was copied and renamed to ensure completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_optimal_retrieval_method = \"eval_datasets/3_optimal_retrieval_method/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_method.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries_from_eval_dataset(json_path: str) -> List[str]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    queries = [entry[\"query\"] for entry in data]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32269868",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create multi query list ###\n",
    "\n",
    "queries = extract_queries_from_eval_dataset(eval_dataset_optimal_retrieval_method)\n",
    "\n",
    "path_to_save = \"multi_query_list_1024_128_optimal_retrieval.json\"\n",
    "multi_queries = create_multi_query_list(queries, 3, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d162bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multi_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create multiple hypothetical documents ###\n",
    "\n",
    "path_to_save = \"hypothetical_documents_1024_128_optimal_retrieval.json\"\n",
    "\n",
    "hypo_documents = create_document_hypo_docs_list(queries, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hypo_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f684f",
   "metadata": {},
   "source": [
    "#### 4.2 Enrich Evaluation Datasets with Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef823c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_methods = [\"TF-IDF\", \"BM25\", \"Dense\", \"MMR\", \"Hypo_Docs\", \"Multiple_Queries\", \"Hybrid_TF-IDF_Dense\", \"Hybrid_BM25_Dense\", \"Hybrid_MMR_BM25\", \"Hybrid_Hypo_Docs\", \"Hybrid_Multiple_Queries\"] \n",
    "\n",
    "enriched_datasets = {}\n",
    "\n",
    "for method in retrieval_methods:\n",
    "    \n",
    "    print(f\"Enriching evaluation dataset for {method} retriever:\")\n",
    "    \n",
    "    enriched = enrich_eval_dataset_with_rag_responses_for_optimal_retrieval(\n",
    "        eval_dataset=eval_dataset_optimal_retrieval_method,\n",
    "        chroma_path=chroma_db_optimal_retrieval_method,\n",
    "        chunk_documents_path=\"1815_documents_for_sparse_retrieval_1024_128_default_baseline.json\",  \n",
    "        k=6,\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        retrieval_method=method,\n",
    "        hypo_document=hypo_documents,\n",
    "        multiple_queries=multi_queries\n",
    "    )\n",
    "\n",
    "    enriched_datasets[method] = enriched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71de28",
   "metadata": {},
   "source": [
    "#### 4.2. Evaluate Retrieval & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c20112",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_methods = [\"TF-IDF\", \"BM25\", \"Dense\", \"MMR\", \"Hypo_Docs\", \"Multi_Queries\", \"Hybrid_TF-IDF_Dense\", \"Hybrid_BM25_Dense\", \"Hybrid_MMR_BM25\", \"Hybrid_Hypo_Docs\", \"Hybrid_Multi_Queries\"]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be753ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_optimal_chunking = {}\n",
    "generation_results_optimal_chunking = {}\n",
    "\n",
    "db_name = chroma_db_optimal_retrieval_method.split(\"/\")[-1]\n",
    "\n",
    "for index, method in enumerate(retrieval_methods):\n",
    "    \n",
    "    json_filename = f\"3_optimal_retrieval_method/{enriched_datasets[method].split('/')[-1]}\"\n",
    "    model_name = f\"optimal_retrieval_{index+1}_{method}_{db_name.replace('_optimal_retrieval', '')}\"\n",
    "\n",
    "    print(f\"\\nEvaluating {model_name} using dataset {json_filename}...\")\n",
    "\n",
    "    retrieval_result = run_retrieval_evaluation(\n",
    "        json_filename=json_filename,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    generation_result = run_generation_evaluation(\n",
    "        json_filename=json_filename,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    evaluation_results_optimal_chunking[model_name] = retrieval_result\n",
    "    generation_results_optimal_chunking[model_name] = generation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder_path = \"eval_results/3_optimal_retrieval_method\"\n",
    "pattern_retrieval = os.path.join(folder_path, \"optimal_retrieval*retrieval_evaluation.csv\")\n",
    "pattern_generation = os.path.join(folder_path, \"optimal_retrieval*generation_evaluation.csv\")\n",
    "csv_retrieval_files = glob.glob(pattern_retrieval)\n",
    "csv_generation_files = glob.glob(pattern_generation)\n",
    "\n",
    "print(f\"Länge Retrieval Files: {len(csv_retrieval_files)}\")\n",
    "print(f\"Länge Generation Files: {len(csv_generation_files)}\")\n",
    "\n",
    "\n",
    "df_retrieval = []\n",
    "df_generation = []\n",
    "\n",
    "for f in csv_retrieval_files:\n",
    "    df = pd.read_csv(f)\n",
    "    df_retrieval.append(df)\n",
    "    \n",
    "for f in csv_generation_files:\n",
    "    df = pd.read_csv(f)\n",
    "    df_generation.append(df)\n",
    "\n",
    "combined_df_retrieval = pd.concat(df_retrieval, ignore_index=True)\n",
    "combined_df_generation = pd.concat(df_generation, ignore_index=True)\n",
    "\n",
    "# Speichern\n",
    "output_path_retrieval = os.path.join(folder_path, \"combined_optimal_retriever_retrieval_evaluation.csv\")\n",
    "output_path_generation = os.path.join(folder_path, \"combined_optimal_retriever_generation_evaluation.csv\")\n",
    "\n",
    "combined_df_retrieval.to_csv(output_path_retrieval, index=False)\n",
    "combined_df_generation.to_csv(output_path_generation, index=False)\n",
    "\n",
    "print(f\"✅ Done! Retrieval: {output_path_retrieval}\\n✅ Generation: {output_path_generation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
