{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aab8476",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation for Optimal Retrieval Process in RAG\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Vector Database\n",
    "3. Querying the Vector Database\n",
    "4. Output of RAG Experiments\n",
    "5. Evaluations\n",
    "\n",
    "This notebook uses functions from the Baseline RAG .ipynb file and adapts these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee75dc",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a150b19",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe95eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# === OpenAI Integration ===\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# === Project Root Configuration ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# === Local Project Modules ===\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import (\n",
    "    load_vector_database,\n",
    "    retrieve_documents,\n",
    "    generate_answer,\n",
    "    rag_pipeline\n",
    ")\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import generate_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349942b",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57123835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables. Assumes that the project directory contains a .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Set client for chat completion \n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma_dbs/chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bbaee",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector Databases\n",
    "\n",
    "**Why Creating Separate Chroma Databases for Each Retrieval Process Is Not Necessary**\n",
    "\n",
    "In contrast to chunking experiments, evaluating different retrieval strategies does not require generating separate Chroma vector databases. This is because all strategies operate over the same underlying document corpus and embeddings. Retrieval processes such as iterative, recursive or adaptive approaches differ only in how they search the embedded documents‚Äînot in how the documents are chunked or stored.\n",
    "\n",
    "As long as the Chroma DB is generated using a consistent chunking strategy and embedding model, it provides a shared semantic space that is sufficient for fair comparison across retrieval methods. Creating separate vector stores per strategy would introduce unnecessary redundancy and would not improve the validity of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_optimal_retrieval_process = \"../chroma_dbs/chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_process\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5f92c",
   "metadata": {},
   "source": [
    "### 3. Retrieval Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5e651",
   "metadata": {},
   "source": [
    "| Retrieval Processes   | Retrieval Method  | Explanation  |\n",
    "|---|---|---|\n",
    "| Iterative  | Dense  | Repeatedly alternates between retrieval and generation, using generated content to refine subsequent retrievals.  |\n",
    "| Recursive  | Dense  | Breaks down complex queries into sub-questions and solves them step by step, often guided by chain-of-thought reasoning  |\n",
    "| Adaptive  | Dense  | Empowers the model to dynamically decide whether, when, and how much to retrieve, based on uncertainty, special tokens, or self-reflection.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f537a3",
   "metadata": {},
   "source": [
    "#### 3.1 Iterative RAG Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f385be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, last_response):\n",
    "    return f\"{original_query}. Hinweis: Beachte bei der Beantwortung auch: {last_response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17500141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity_with_embeddings(text1, text2, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between OpenAI embeddings of two texts.\n",
    "    \"\"\"\n",
    "    embeddings = openai.embeddings.create(\n",
    "        model=model,\n",
    "        input=[text1, text2]\n",
    "    )\n",
    "    \n",
    "    vec1 = np.array(embeddings.data[0].embedding)\n",
    "    vec2 = np.array(embeddings.data[1].embedding)\n",
    "    \n",
    "    return float(cosine_similarity([vec1], [vec2])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_iterative(query, database, k, model_name, max_iterations=3, similarity_threshold=0.95):\n",
    "    current_query = query\n",
    "    retrieved_sources_accumulated = []\n",
    "    retrieved_contexts_accumulated = []\n",
    "    retrieved_ids_accumulated = []\n",
    "    retrieved_indices_accumulated = []\n",
    "    previous_response = \"\"\n",
    "    retrieved_ids_set = set() \n",
    "\n",
    "    for iteration in trange(max_iterations, desc=\"Iterative RAG\"):\n",
    "        \n",
    "        # Retriever\n",
    "        results = retrieve_documents(query_text=current_query, db=database, k=k)        \n",
    "\n",
    "        for doc, _ in results:\n",
    "            chunk_id = doc.metadata.get(\"chunk_id\")\n",
    "            if chunk_id not in retrieved_ids_set:\n",
    "                retrieved_contexts_accumulated.append(doc.page_content)\n",
    "                retrieved_sources_accumulated.append(doc.metadata.get(\"source\"))\n",
    "                retrieved_ids_accumulated.append(chunk_id)\n",
    "                retrieved_indices_accumulated.append(doc.metadata.get(\"chunk_index\"))\n",
    "                retrieved_ids_set.add(chunk_id)  \n",
    "\n",
    "        # Generator\n",
    "        response = generate_answer(results, current_query, model_name)\n",
    "\n",
    "        # Convergence check: similarity of the previously generated response\n",
    "        if previous_response:\n",
    "            similarity = compute_cosine_similarity_with_embeddings(response.strip(), previous_response.strip())\n",
    "            if similarity >= similarity_threshold:\n",
    "                print(f\"\\nCancel at iteration {iteration+1}: Similarity = {similarity:.2f}\")\n",
    "                break\n",
    "\n",
    "        previous_response = response\n",
    "\n",
    "        # Update query with rewrite function\n",
    "        current_query = rewrite_query(query, response)\n",
    "    \n",
    "    return response.strip(), retrieved_sources_accumulated, retrieved_contexts_accumulated, retrieved_ids_accumulated, retrieved_indices_accumulated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2da9e1",
   "metadata": {},
   "source": [
    "#### 3.2 Recursive RAG Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_queries(query, max_depth=3, model_name=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Uses a Chain-of-Thought prompt to generate sub-questions for recursive reasoning.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "                Du bist ein hilfreicher, juristischer KI-Assistent f√ºr Gesetzestexte im deutschen Energie- und Versorgungsbereich. Zerlege die folgende komplexe Frage in {max_depth} einfachere, kurze Teilfragen, die Schritt f√ºr Schritt bei der Beantwortung helfen.\n",
    "\n",
    "                Frage:\n",
    "                {query}\n",
    "\n",
    "                Liste jede Teilfrage in einer neuen Zeile auf:\n",
    "            \"\"\"\n",
    "            \n",
    "    model = ChatOpenAI(model_name=model_name)\n",
    "    response = model.predict(prompt)\n",
    "    return [q.strip() for q in response.strip().split(\"\\n\") if q.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_recursive_answers(original_query, sub_answers, model_name):\n",
    "    \"\"\"\n",
    "    Aggregates all answers of the sub-queries to a consistent answer.\n",
    "    \"\"\"\n",
    "    combined = \"\\n\".join([f\"Frage: {q}\\nAntwort: {a}\" for q, a in sub_answers])\n",
    "    prompt = f\"\"\"\n",
    "                Basierend auf der urspr√ºnglichen Frage: \"{original_query}\"\n",
    "                und den folgenden Teilantworten, generiere eine kurze, pr√§zise, in sich schl√ºssige und vollst√§ndige Gesamtantwort:\n",
    "\n",
    "                {combined}\n",
    "                \n",
    "                Die Antwort soll sehr kurz sein mit einer maximalen Tokenl√§nge von 200.\n",
    "            \"\"\"\n",
    "            \n",
    "    model = ChatOpenAI(model_name=model_name)\n",
    "    response = model.predict(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d630fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sub_query(sub_query, db, k, model_name):\n",
    "    # Retrieval\n",
    "    results = retrieve_documents(query_text=sub_query, db=db, k=k)\n",
    "\n",
    "    # Antwort generieren\n",
    "    answer = generate_answer(results, sub_query, model_name)\n",
    "\n",
    "    return sub_query, answer, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def rag_pipeline_recursive(query: str, database, k: int, model_name: str, max_depth: int = 3, max_iterations: int = 3, convergence_threshold: float = 0.95):\n",
    "\n",
    "    retrieved_sources_accumulated = []\n",
    "    retrieved_contexts_accumulated = []\n",
    "    retrieved_ids_accumulated = []\n",
    "    retrieved_ids_set = set()\n",
    "    retrieved_indices_accumulated = []\n",
    "    \n",
    "    previous_response = \"\"\n",
    "    current_query = query\n",
    "\n",
    "    for iteration in tqdm(range(max_iterations), desc=\"Recursive RAG\"):\n",
    "        \n",
    "        sub_answers = []\n",
    "        sub_queries = generate_sub_queries(current_query, model_name=model_name, max_depth=max_depth)\n",
    "\n",
    "\n",
    "        # Build preconfigured function\n",
    "        process_fn = partial(process_sub_query, db=database, k=k, model_name=model_name)\n",
    "\n",
    "        # Parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(process_fn, sub_query) for sub_query in sub_queries]\n",
    "\n",
    "            for future in futures:\n",
    "                sub_query, answer, results = future.result()\n",
    "                \n",
    "\n",
    "                for doc, _ in results:\n",
    "                    chunk_id = doc.metadata.get(\"chunk_id\")\n",
    "                    if chunk_id not in retrieved_ids_set:\n",
    "                        retrieved_contexts_accumulated.append(doc.page_content)\n",
    "                        retrieved_sources_accumulated.append(doc.metadata.get(\"source\"))\n",
    "                        retrieved_ids_accumulated.append(chunk_id)\n",
    "                        retrieved_indices_accumulated.append(doc.metadata.get(\"chunk_index\"))\n",
    "                        retrieved_ids_set.add(chunk_id)\n",
    "\n",
    "                sub_answers.append((sub_query, answer))\n",
    "                        \n",
    "        response = aggregate_recursive_answers(query, sub_answers, model_name)\n",
    "\n",
    "        # Convergence check: similarity of the previously generated response\n",
    "        if previous_response:\n",
    "            similarity = compute_cosine_similarity_with_embeddings(response.strip(), previous_response.strip())\n",
    "            if similarity >= convergence_threshold:\n",
    "                print(f\"\\nCancel at iteration {iteration+1}: Similarity = {similarity:.2f}\")\n",
    "                break\n",
    "\n",
    "        previous_response = response\n",
    "        current_query = rewrite_query(query, response)\n",
    "\n",
    "    return response.strip(), retrieved_sources_accumulated, retrieved_contexts_accumulated, retrieved_ids_accumulated, retrieved_indices_accumulated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881caaf",
   "metadata": {},
   "source": [
    "#### 3.3. Adapative RAG Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_judge(query: str, answer: str, model_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Das Modell reflektiert, ob die Antwort gut genug ist.\n",
    "    R√ºckgabe: True = Antwort reicht aus, False = Retrieval n√∂tig\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Die folgende Antwort wurde auf die Frage gegeben: \"{query}\"\n",
    "\n",
    "    Antwort:\n",
    "    {answer}\n",
    "\n",
    "    Beurteile, ob die Antwort vollst√§ndig, korrekt und nachvollziehbar ist.\n",
    "    Wenn die Antwort unzureichend ist und eine zus√§tzliche Recherche notwendig w√§re, antworte mit \"RETRIEVE\".\n",
    "    Andernfalls antworte mit \"OK\".\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model_name=model_name)\n",
    "    response = model.predict(prompt).strip().upper()\n",
    "    \n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    return response != \"OK\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c855ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_adaptive(query: str, database, k: int, model_name: str, max_depth: int = 3, max_iterations: int = 3, convergence_threshold: float = 0.95):\n",
    "    \"\"\"\n",
    "    Adaptive RAG: Erst direkt generieren, dann ggf. Retrieval triggern (Self-RAG Light)\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model_name=model_name)\n",
    "\n",
    "    # Schritt 1: Direkte Antwort\n",
    "    direct_prompt = f\"Beantworte die folgende Frage so gut wie m√∂glich: {query}\"\n",
    "    initial_answer = model.predict(direct_prompt).strip()\n",
    "\n",
    "    # Schritt 2: Reflektiere, ob Antwort ausreicht\n",
    "    needs_retrieval = reflection_judge(query, initial_answer, model_name)\n",
    "\n",
    "    if not needs_retrieval:\n",
    "        return initial_answer, [], [], [], []\n",
    "\n",
    "    # Schritt 3: Falls n√∂tig ‚Üí Retrieval (z.‚ÄØB. recursive_rag)\n",
    "    retrieved_sources_accumulated = []\n",
    "    retrieved_contexts_accumulated = []\n",
    "    retrieved_ids_accumulated = []\n",
    "    retrieved_ids_set = set()\n",
    "    retrieved_indices_accumulated = []\n",
    "    \n",
    "    previous_response = \"\"\n",
    "    current_query = query\n",
    "\n",
    "    for iteration in tqdm(range(max_iterations), desc=\"Recursive RAG\"):\n",
    "                \n",
    "        sub_answers = []\n",
    "        sub_queries = generate_sub_queries(current_query, model_name=model_name, max_depth=max_depth)\n",
    "\n",
    "\n",
    "        # Build preconfigured function\n",
    "        process_fn = partial(process_sub_query, db=database, k=k, model_name=model_name)\n",
    "\n",
    "        # Parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(process_fn, sub_query) for sub_query in sub_queries]\n",
    "\n",
    "            for future in futures:\n",
    "                sub_query, answer, results = future.result()\n",
    "\n",
    "                for doc in results:\n",
    "                    chunk_id = doc.metadata.get(\"chunk_id\")\n",
    "                    if chunk_id not in retrieved_ids_set:\n",
    "                        retrieved_contexts_accumulated.append(doc.page_content)\n",
    "                        retrieved_sources_accumulated.append(doc.metadata.get(\"source\"))\n",
    "                        retrieved_ids_accumulated.append(chunk_id)\n",
    "                        retrieved_indices_accumulated.append(doc.metadata.get(\"chunk_index\"))\n",
    "                        retrieved_ids_set.add(chunk_id)\n",
    "\n",
    "                sub_answers.append((sub_query, answer))\n",
    "                        \n",
    "        response = aggregate_recursive_answers(query, sub_answers, model_name)\n",
    "        \n",
    "        if not reflection_judge(query, response, model_name):\n",
    "            print(f\"‚úÖ Iteration {iteration + 1}: Answer sufficient, cancel retrieval and generate final answer\")\n",
    "            break\n",
    "\n",
    "        # Convergence check: similarity of the previously generated response\n",
    "        if previous_response:\n",
    "            similarity = compute_cosine_similarity_with_embeddings(response.strip(), previous_response.strip())\n",
    "            if similarity >= convergence_threshold:\n",
    "                print(f\"\\nCancel at iteration {iteration+1}: Similarity = {similarity:.2f}\")\n",
    "                break\n",
    "\n",
    "        previous_response = response\n",
    "        current_query = rewrite_query(query, response)\n",
    "\n",
    "    return response.strip(), retrieved_sources_accumulated, retrieved_contexts_accumulated, retrieved_ids_accumulated, retrieved_indices_accumulated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6e47c",
   "metadata": {},
   "source": [
    "### 4. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e51dc",
   "metadata": {},
   "source": [
    "#### 4.1 Preparing the Evaluation Dataset\n",
    "\n",
    "Since no new Chroma DB had to be created, the evaluation data set from the RAG baseline can also be reused. The data set was copied and renamed to ensure completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_optimal_retrieval_process = \"eval_datasets/4_optimal_retrieval_process/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_process.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f684f",
   "metadata": {},
   "source": [
    "#### 4.2 Enrich Evaluation Datasets with Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39535ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_rag_responses_for_optimal_retrieval(\n",
    "    eval_dataset, \n",
    "    chroma_path, \n",
    "    k, \n",
    "    model_name, \n",
    "    rag_mode=\"baseline\",  # baseline | iterative | recursive | adaptive\n",
    "    optimization=\"4_optimal_retrieval_process\"\n",
    "):\n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "\n",
    "    for i, entry in enumerate(tqdm(eval_dataset_json, desc=\"Processing RAG responses\")):\n",
    "        query = entry[\"query\"]\n",
    "\n",
    "        # Dynamische Auswahl der Pipeline\n",
    "        if rag_mode == \"baseline\":\n",
    "            response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline(\n",
    "                query, db, model_name)\n",
    "        elif rag_mode == \"iterative\":\n",
    "            response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline_iterative(\n",
    "                query, db, k, model_name\n",
    "            )\n",
    "        elif rag_mode == \"recursive\":\n",
    "            response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline_recursive(\n",
    "                query, db, k, model_name\n",
    "            )\n",
    "        elif rag_mode == \"adaptive\":\n",
    "            response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline_adaptive(\n",
    "                query, db, k, model_name\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RAG mode: {rag_mode}\")\n",
    "\n",
    "        # Ergebnisse hinzuf√ºgen\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "        enriched_dataset.append(entry)\n",
    "\n",
    "    output_path = f\"eval_datasets/{optimization}{eval_dataset.split('/')[-1].replace('.json', '')}_{rag_mode}_rag_enriched.json\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef823c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_processes = [\"iterative\", \"recursive\", \"adaptive\"]\n",
    "\n",
    "enriched_datasets = {}\n",
    "\n",
    "for process in rag_processes:\n",
    "    \n",
    "    print(f\"Enriching evaluation dataset for {process} RAG:\")\n",
    "    \n",
    "    enriched = enrich_eval_dataset_with_rag_responses_for_optimal_retrieval(\n",
    "        eval_dataset=eval_dataset_optimal_retrieval_process,\n",
    "        chroma_path=chroma_db_optimal_retrieval_process,\n",
    "        k=6,\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        rag_mode=process\n",
    "    )\n",
    "\n",
    "    enriched_datasets[process] = enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_processes = [\"iterative\", \"recursive\"]\n",
    "\n",
    "enriched_datasets = {\n",
    "    'iterative': 'eval_datasets/4_optimal_retrieval_process/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_process_iterative_rag_enriched.json', \n",
    "    'recursive': 'eval_datasets/4_optimal_retrieval_process/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_process_recursive_rag_enriched.json', \n",
    "    'adaptive': 'eval_datasets/4_optimal_retrieval_process/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_retrieval_process_adaptive_rag_enriched.json'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71de28",
   "metadata": {},
   "source": [
    "#### 4.2. Evaluate Retrieval & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be753ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_optimal_chunking = {}\n",
    "generation_results_optimal_chunking = {}\n",
    "\n",
    "db_name = chroma_db_optimal_retrieval_process.split(\"/\")[-1]\n",
    "\n",
    "for index, process in enumerate(rag_processes):\n",
    "\n",
    "    json_filename = f\"4_optimal_retrieval_process/{enriched_datasets[process].split('/')[-1]}\"\n",
    "    model_name = f\"optimal_retrieval_{index+1}_{process}_{db_name.replace('_optimal_retrieval_process', '')}\"\n",
    "\n",
    "\n",
    "    print(f\"\\nEvaluating {model_name} using dataset {json_filename}...\")\n",
    "\n",
    "    retrieval_result = run_retrieval_evaluation(\n",
    "        json_filename=json_filename,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    generation_result = run_generation_evaluation(\n",
    "        json_filename=json_filename,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    evaluation_results_optimal_chunking[model_name] = retrieval_result\n",
    "    generation_results_optimal_chunking[model_name] = generation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd285ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = \"adaptive\"\n",
    "json_filename = f\"4_optimal_retrieval_process/{enriched_datasets[process].split('/')[-1]}\"\n",
    "model_name = f\"optimal_retrieval_{3}_{process}_{db_name.replace('_optimal_retrieval_process', '')}\"\n",
    "\n",
    "\n",
    "print(f\"\\nEvaluating {model_name} using dataset {json_filename}...\")\n",
    "\n",
    "generation_result = run_generation_evaluation(\n",
    "    json_filename=json_filename,\n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define base folder and file patterns\n",
    "folder_path = Path(\"eval_results\") / \"4_optimal_retrieval_process\"\n",
    "pattern_retrieval = \"optimal_retrieval*retrieval_evaluation.csv\"\n",
    "pattern_generation = \"optimal_retrieval*generation_evaluation.csv\"\n",
    "\n",
    "# Find matching CSV files\n",
    "csv_retrieval_files = list(folder_path.glob(pattern_retrieval))\n",
    "csv_generation_files = list(folder_path.glob(pattern_generation))\n",
    "\n",
    "print(f\"üîç Found {len(csv_retrieval_files)} retrieval files.\")\n",
    "print(f\"üîç Found {len(csv_generation_files)} generation files.\")\n",
    "\n",
    "# Load and combine retrieval evaluation files\n",
    "df_retrieval = [pd.read_csv(f) for f in csv_retrieval_files]\n",
    "df_generation = [pd.read_csv(f) for f in csv_generation_files]\n",
    "\n",
    "# Concatenate if there is at least one file\n",
    "if df_retrieval:\n",
    "    combined_df_retrieval = pd.concat(df_retrieval, ignore_index=True)\n",
    "    output_path_retrieval = folder_path / \"combined_optimal_retrieval_process_retrieval_evaluation.csv\"\n",
    "    combined_df_retrieval.to_csv(output_path_retrieval, index=False)\n",
    "    print(f\"‚úÖ Retrieval results saved to: {output_path_retrieval}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No retrieval CSV files found.\")\n",
    "\n",
    "if df_generation:\n",
    "    combined_df_generation = pd.concat(df_generation, ignore_index=True)\n",
    "    output_path_generation = folder_path / \"combined_optimal_retrieval_process_generation_evaluation.csv\"\n",
    "    combined_df_generation.to_csv(output_path_generation, index=False)\n",
    "    print(f\"‚úÖ Generation results saved to: {output_path_generation}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No generation CSV files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cd2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
