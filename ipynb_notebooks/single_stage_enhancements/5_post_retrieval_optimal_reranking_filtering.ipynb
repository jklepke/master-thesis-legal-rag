{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5e2340",
   "metadata": {},
   "source": [
    "# Implementation, Testing and Evaluation for Optimal Post-Retrieval in RAG\n",
    "\n",
    "#### Notebook Outline\n",
    "1. Imports and Configurations\n",
    "2. Creation of Vector Database\n",
    "3. Querying the Vector Database\n",
    "4. Output of RAG Experiments\n",
    "5. Evaluations\n",
    "\n",
    "This notebook uses functions from the Baseline RAG .ipynb file and adapts these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4749e",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fc96c",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# === Scientific and Utility Libraries ===\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Union, Tuple\n",
    "\n",
    "# === Language Detection and Text Similarity ===\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Transformers and Sentence Embeddings ===\n",
    "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "# === OpenAI Integrations ===\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# === LangChain Core Components ===\n",
    "from langchain.schema import Document\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# === Project Root Configuration ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# === Custom Project Modules ===\n",
    "from ipynb_notebooks.single_stage_enhancements.rankGPT_rerank import rankgpt_rerank\n",
    "\n",
    "from ipynb_notebooks.baseline.rag_utils.baseline_rag import (\n",
    "    load_vector_database,\n",
    "    retrieve_documents,\n",
    "    generate_answer,\n",
    "    translate_query_to_german_if_needed,\n",
    "    detect_language_name\n",
    ")\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.eval_vector_dataset_generator import generate_evalset\n",
    "from ipynb_notebooks.evaluation_datasets.retrieval_eval.retrieval_metrics import run_retrieval_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36ae5c",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables. Assumes that the project directory contains a .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "\n",
    "# Cohere Client\n",
    "\n",
    "# Define constants for paths\n",
    "DATA_PATH = \"../../data/laws_and_ordinances.json\"  # Directory containing the url to the law and ordinance documents\n",
    "DATA_PATH_SHORT_VERSION = \"../../data/laws_and_ordinances_short_version.json\" # Directory containing a subset of all urls for testing purposes\n",
    "CHROMA_PATH = \"chroma_dbs/chroma\"  # Directory to save the Chroma vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc26986",
   "metadata": {},
   "source": [
    "### 2. Creation of Vector Databases\n",
    "\n",
    "**Why Creating Separate Chroma Databases for Each Retrieval Process Is Not Necessary**\n",
    "\n",
    "In contrast to chunking experiments, evaluating different retrieval strategies does not require generating separate Chroma vector databases. This is because all strategies operate over the same underlying document corpus and embeddings. Retrieval processes such as iterative, recursive or adaptive approaches differ only in how they search the embedded documents‚Äînot in how the documents are chunked or stored.\n",
    "\n",
    "As long as the Chroma DB is generated using a consistent chunking strategy and embedding model, it provides a shared semantic space that is sufficient for fair comparison across retrieval methods. Creating separate vector stores per strategy would introduce unnecessary redundancy and would not improve the validity of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ba732",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_optimal_reranking = \"../chroma_dbs/chroma_chunksize1024_overlap128_c800ccc6_optimal_reranking\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36042a7",
   "metadata": {},
   "source": [
    "### 3. Post-Retrieval Optimization: Filtering, Reranking, Summarizing & Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b97dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents(\n",
    "    results: Union[List[Document], List[Tuple[Document, float]]],\n",
    "    query: str,\n",
    "    score_threshold: float = 0.25\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filters documents based on relevance scores or cosine similarity using normalized scores.\n",
    "\n",
    "    Args:\n",
    "        results: List of Documents or (Document, Score) tuples.\n",
    "        query: The search query string.\n",
    "        score_threshold: Normalized similarity threshold between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        List of Documents that pass the similarity threshold.\n",
    "    \"\"\"\n",
    "    filtered_docs = []\n",
    "\n",
    "    # Case 1: Scores are already provided\n",
    "    if results and isinstance(results[0], tuple):\n",
    "        scores = [score for _, score in results]\n",
    "        min_score, max_score = min(scores), max(scores)\n",
    "\n",
    "        for doc, score in results:\n",
    "            # Normalize score\n",
    "            norm_score = (score - min_score) / (max_score - min_score + 1e-8)\n",
    "            if norm_score >= score_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "    else:\n",
    "        docs = results\n",
    "        doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "        embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        # Embed query and documents\n",
    "        query_vec = embedding_model.embed_query(text=query)\n",
    "        doc_vecs = embedding_model.embed_documents(texts=doc_texts)\n",
    "\n",
    "        # Compute similarity\n",
    "        similarity_scores = cosine_similarity([query_vec], doc_vecs)[0]\n",
    "\n",
    "        # Normalize\n",
    "        min_score, max_score = similarity_scores.min(), similarity_scores.max()\n",
    "\n",
    "        for doc, score in zip(docs, similarity_scores):\n",
    "            norm_score = (score - min_score) / (max_score - min_score + 1e-8)\n",
    "            if norm_score >= score_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "    return filtered_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d850c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc_order(title, docs):\n",
    "    indices = [doc.metadata.get(\"chunk_index\", \"N/A\") for doc in docs]\n",
    "    print(f\"\\n{title}: {indices}\")\n",
    "\n",
    "\n",
    "def rerank_with_bge(docs, query, top_k, doc_order=False):\n",
    "    if doc_order:\n",
    "        print_doc_order(\"Before Reranking (BGE)\", docs)\n",
    "\n",
    "    inputs = [(doc.page_content, query) for doc in docs]\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-base')\n",
    "    scores = reranker.compute_score(inputs, batch_size=32)\n",
    "    scored_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "    ranked_docs = [doc for _, doc in scored_docs[:top_k]]\n",
    "\n",
    "    if doc_order:\n",
    "        print_doc_order(\"After Reranking (BGE)\", ranked_docs)\n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "def rerank_with_cohere(docs, query, top_k, doc_order=False):\n",
    "    if doc_order:\n",
    "        print_doc_order(\"Before Reranking (Cohere)\", docs)\n",
    "\n",
    "    compressor = CohereRerank(top_n=top_k, user_agent=\"langchain\", model=\"rerank-english-v3.0\")\n",
    "    ranked_docs = compressor.compress_documents(documents=docs, query=query)\n",
    "    time.sleep(7)\n",
    "\n",
    "    if doc_order:\n",
    "        print_doc_order(\"After Reranking (Cohere)\", ranked_docs)\n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "def rerank_with_colbert(docs, query, top_k, device, doc_order=False):\n",
    "    if doc_order:\n",
    "        print_doc_order(\"Before Reranking (ColBERT)\", docs)\n",
    "\n",
    "    model_name = \"colbert-ir/colbertv2.0\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_tokens = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        query_embeds = model(**query_tokens).last_hidden_state.squeeze(0)\n",
    "\n",
    "        scores = []\n",
    "        for doc in docs:\n",
    "            doc_tokens = tokenizer(doc.page_content, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            doc_embeds = model(**doc_tokens).last_hidden_state.squeeze(0)\n",
    "            sim_matrix = torch.matmul(query_embeds, doc_embeds.T)\n",
    "            max_sim = sim_matrix.max(dim=1).values.sum().item()\n",
    "            scores.append((max_sim, doc))\n",
    "\n",
    "    ranked_docs = [doc for score, doc in sorted(scores, key=lambda x: x[0], reverse=True)][:top_k]\n",
    "\n",
    "    if doc_order:\n",
    "        print_doc_order(\"After Reranking (ColBERT)\", ranked_docs)\n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "\n",
    "def rerank_with_monot5(docs, query, top_k, device, doc_order=False):\n",
    "    if doc_order:\n",
    "        print_doc_order(\"Before Reranking (MonoT5)\", docs)\n",
    "\n",
    "    model_name = \"castorini/monot5-base-msmarco\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    inputs = [f\"Query: {query} Document: {doc.page_content} Relevant:\" for doc in docs]\n",
    "    encodings = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**encodings, max_length=2)\n",
    "\n",
    "    scores = []\n",
    "    for output, doc in zip(outputs, docs):\n",
    "        label = tokenizer.decode(output, skip_special_tokens=True).strip().lower()\n",
    "        score = 1.0 if label == \"true\" else 0.0\n",
    "        scores.append((score, doc))\n",
    "\n",
    "    ranked_docs = [doc for score, doc in sorted(scores, key=lambda x: x[0], reverse=True)][:top_k]\n",
    "\n",
    "    if doc_order:\n",
    "        print_doc_order(\"After Reranking (MonoT5)\", ranked_docs)\n",
    "\n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "def rerank_with_rankgpt(docs, query, model_name, doc_order=False):\n",
    "    if doc_order:\n",
    "        print_doc_order(\"Before Reranking (RankGPT)\", docs)\n",
    "\n",
    "    reranked_docs = rankgpt_rerank(query, docs, model_name=model_name, window_size=4, step=1)\n",
    "\n",
    "    if doc_order:\n",
    "        print_doc_order(\"After Reranking (RankGPT)\", reranked_docs)\n",
    "\n",
    "    return reranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(results, query, top_k=5, model_name=\"gpt-4o-mini\", reranker_type=\"cohere\", device=\"cpu\", doc_order=False):\n",
    "    # Extract documents from results (handle tuple format if necessary)\n",
    "    docs_only = [doc for doc, _ in results] if results and isinstance(results[0], tuple) else results or []\n",
    "\n",
    "    # Define the reranker dispatcher\n",
    "    reranker_dispatcher = {\n",
    "                            \"bge\": lambda: rerank_with_bge(docs_only, query, top_k, doc_order),\n",
    "                            \"cohere\": lambda: rerank_with_cohere(docs_only, query, top_k, doc_order),\n",
    "                            \"colbert\": lambda: rerank_with_colbert(docs_only, query, top_k, device, doc_order),\n",
    "                            \"monot5\": lambda: rerank_with_monot5(docs_only, query, top_k, device, doc_order),\n",
    "                            \"LLM_reranker_rankGPT\": lambda: rerank_with_rankgpt(docs_only, query, model_name, doc_order)\n",
    "                            }\n",
    "\n",
    "\n",
    "    if reranker_type not in reranker_dispatcher:\n",
    "        raise ValueError(f\"Unknown reranker_type '{reranker_type}' provided.\")\n",
    "\n",
    "    return reranker_dispatcher[reranker_type]()  # Execute selected reranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.schema import Document\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def summarize_single_top_chunk(doc: Document, query: str, model_name: str = \"gpt-4o-mini\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a single top chunk with detailed focus in relation to the query.\n",
    "    Includes law_title or title from metadata.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    title = doc.metadata.get(\"law_title\") or doc.metadata.get(\"title\", \"Unbekannter Titel\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Du bist ein juristischer KI-Assistent. Fasse den folgenden Gesetzesauszug aus dem Dokument \"{title}\" \n",
    "so zusammen, dass alle relevanten Informationen in Bezug auf die Frage enthalten sind.\n",
    "\n",
    "Frage: {query}\n",
    "\n",
    "--- Gesetzestext ---\n",
    "{doc.page_content}\n",
    "--- Ende Gesetzestext ---\n",
    "\n",
    "Zusammenfassung (max. 5 Stichpunkte):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return f\"Chunk Titel: {title}\\n Zusammenfassung: {response.choices[0].message.content.strip()}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing top chunk: {e}\")\n",
    "        return f\"TITEL: {title}\\nFehler beim Zusammenfassen dieses Chunks.\"\n",
    "\n",
    "def summarize_top_chunks_with_query_parallel(\n",
    "    top_chunks: List[Document],\n",
    "    query: str,\n",
    "    model_name: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    max_workers: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Summarizes each top chunk in parallel and returns list of their summaries with title.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(summarize_single_top_chunk, doc, query, model_name, temperature)\n",
    "            for doc in top_chunks\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                summary = future.result()\n",
    "                summaries.append(summary)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during summarization of a top chunk: {e}\")\n",
    "                summaries.append(\"\")\n",
    "\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2629dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_support_chunks_combined(\n",
    "    support_chunks: List[Document],\n",
    "    query: str,\n",
    "    model_name: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Combines all support chunks and summarizes them with a single LLM call.\n",
    "    Focuses on extracting only the most relevant information to the query.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Combine context into one string with titles\n",
    "    combined_text = \"\"\n",
    "    for doc in support_chunks:\n",
    "        title = doc.metadata.get(\"law_title\") or doc.metadata.get(\"title\", \"Unbekannter Titel\")\n",
    "        combined_text += f\"\\nTITEL: {title}\\n{doc.page_content}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Du bist ein juristischer KI-Assistent. Extrahiere aus den folgenden Gesetzestexten nur die Informationen,\n",
    "die direkt relevant f√ºr die folgende Frage sind. Die Antwort soll kurz, strukturiert und pr√§zise sein.\n",
    "\n",
    "Frage: {query}\n",
    "\n",
    "--- Gesetzestexte ---\n",
    "{combined_text}\n",
    "--- Ende Gesetzestexte ---\n",
    "\n",
    "Antwort (in maximal 10 Stichpunkten):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing support chunks: {e}\")\n",
    "        return \"Fehler beim Zusammenfassen der unterst√ºtzenden Chunks.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb323121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def summarizing_and_prompt_engineering(\n",
    "    retrieved_chunks: List[Document],\n",
    "    query: str,\n",
    "    model_name: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Builds the final prompt using summarizations from top and support chunks.\n",
    "\n",
    "    Top chunks are summarized in parallel (fokus: Genauigkeit), \n",
    "    support chunks in einem gemeinsamen Aufruf (fokus: Extraktion relevanter Infos).\n",
    "    \"\"\"\n",
    "    \n",
    "    query_de = translate_query_to_german_if_needed(query)\n",
    "    detected_language = detect_language_name(query)\n",
    "    \n",
    "    # split into top chunks and support chunks for summary\n",
    "    n_top_chunks = 2\n",
    "    top_chunks = retrieved_chunks[:n_top_chunks]\n",
    "    support_chunks = retrieved_chunks[n_top_chunks:]\n",
    "\n",
    "    # Summarize top chunks with query context (parallel)\n",
    "    top_chunk_summaries = summarize_top_chunks_with_query_parallel(\n",
    "        top_chunks=top_chunks,\n",
    "        query=query_de,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Summarize support chunks in a single call\n",
    "    support_summary = summarize_support_chunks_combined(\n",
    "        support_chunks=support_chunks,\n",
    "        query=query_de,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Construct context section\n",
    "    context_block = \"\\n\\n---\\n\".join(top_chunk_summaries)\n",
    "    if support_summary:\n",
    "        context_block += \"\\n\\n--- Unterst√ºtzender Kontext ---\\n\" + support_summary\n",
    "\n",
    "    # Final prompt\n",
    "    prompt_template = f\"\"\"\n",
    "        Du bist ein hilfreicher, juristischer KI-Assistent f√ºr Gesetzestexte im deutschen Energie- und Versorgungsbereich. \n",
    "        Generiere eine kurze, pr√§zise, konsistente und vollst√§ndige Gesamtantwort von max. 200 Tokens basierend auf folgendem Kontext:\n",
    "\n",
    "        Frage:\n",
    "        {query}\n",
    "        ---\n",
    "        Kontext:\n",
    "        {context_block}\n",
    "        ---\n",
    "        Sprache in der geantwortet werden soll: \n",
    "        {detected_language}\n",
    "        \"\"\"\n",
    "\n",
    "    return prompt_template.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_post_retrieval(\n",
    "    query,\n",
    "    database,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    filtering: bool = False,\n",
    "    reranking: bool = False,\n",
    "    reranker_type: str = \"cohere\",\n",
    "    summarizing_prompt_engineering: bool = False,\n",
    "    k: int = 6,\n",
    "    thresh_hold: float = 0.75,\n",
    "    doc_order: bool = False\n",
    "):\n",
    "    # Document Retrieval\n",
    "    retrieved_results = retrieve_documents(query, db=database, k=k)\n",
    "\n",
    "    # Filtering\n",
    "    if filtering:\n",
    "        retrieved_results = filter_documents(results=retrieved_results, query=query, score_threshold=thresh_hold)\n",
    "\n",
    "    # Reranking\n",
    "    if reranking:\n",
    "        retrieved_results = rerank_documents(\n",
    "            retrieved_results, query, top_k=5, model_name=model_name, reranker_type=reranker_type, doc_order=doc_order\n",
    "        )\n",
    "\n",
    "    # Summarization & Prompt Engineering\n",
    "    if summarizing_prompt_engineering:\n",
    "        \n",
    "        if retrieved_results and isinstance(retrieved_results[0], tuple):\n",
    "            retrieved_results = [doc for doc, _ in retrieved_results]\n",
    "            \n",
    "        prompt = summarizing_and_prompt_engineering(\n",
    "            retrieved_chunks=retrieved_results,\n",
    "            query=query,\n",
    "            model_name=model_name,\n",
    "            temperature=0.2  \n",
    "        )\n",
    "        response = generate_answer(results=prompt, query_text=query, model_name=model_name)\n",
    "    else:\n",
    "        # Normal generation with plain concatenation of chunks\n",
    "        response = generate_answer(results=retrieved_results, query_text=query, model_name=model_name)\n",
    "\n",
    "    # Extract metadata for logging/tracing\n",
    "    sources = [doc.metadata.get(\"source\") for doc in retrieved_results]\n",
    "    retrieved_chunk_contexts = [doc.page_content for doc in retrieved_results]\n",
    "    retrieved_chunk_ids = [doc.metadata.get(\"chunk_id\") for doc in retrieved_results]\n",
    "    retrieved_chunk_indices = [doc.metadata.get(\"chunk_index\") for doc in retrieved_results]\n",
    "\n",
    "    return response, sources, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f25447",
   "metadata": {},
   "source": [
    "### 4. Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e2968",
   "metadata": {},
   "source": [
    "#### 4.1 Preparing the Evaluation Dataset\n",
    "\n",
    "Since no new Chroma DB had to be created, the evaluation data set from the RAG baseline can also be reused. The data set was copied and renamed to ensure completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ac0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_optimal_reranking = \"eval_datasets/5_optimal_reranking/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_reranking.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8b43e",
   "metadata": {},
   "source": [
    "#### 4.2 Enrich Evaluation Datasets with Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854eafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_eval_dataset_with_rag_responses_for_optimal_reranking(eval_dataset, chroma_path, config, k, model_name, reranker_type=\"cohere\", optimization=\"5_optimal_reranking/\", thresh_hold=0.75, doc_order=False):\n",
    "    \n",
    "    db = load_vector_database(chroma_path)\n",
    "\n",
    "    with open(eval_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset_json = json.load(f)\n",
    "\n",
    "    enriched_dataset = []\n",
    "    \n",
    "    for i, entry in enumerate(tqdm(eval_dataset_json, desc=\"Processing RAG responses\")):\n",
    "        query = entry[\"query\"]\n",
    "\n",
    "        # F√ºhre die RAG-Pipeline aus\n",
    "        response, _, retrieved_chunk_contexts, retrieved_chunk_ids, retrieved_chunk_indices = rag_pipeline_post_retrieval(\n",
    "                query=query,\n",
    "                database=db,\n",
    "                model_name=model_name,\n",
    "                filtering=config[\"filtering\"],\n",
    "                reranking=config[\"reranking\"],\n",
    "                reranker_type=reranker_type,  # oder z.B. cohere\n",
    "                summarizing_prompt_engineering=config[\"summarizing_prompt_engineering\"],\n",
    "                k=k,\n",
    "                thresh_hold=thresh_hold,\n",
    "                doc_order=doc_order\n",
    "            )\n",
    "\n",
    "        # F√ºge neue Felder zur Entry hinzu\n",
    "        entry[\"generated_response\"] = response\n",
    "        entry[\"retrieved_chunk_contexts\"] = retrieved_chunk_contexts\n",
    "        entry[\"retrieved_chunk_ids\"] = retrieved_chunk_ids\n",
    "        entry[\"retrieved_chunk_indices\"] = retrieved_chunk_indices\n",
    "\n",
    "        enriched_dataset.append(entry)\n",
    "        \n",
    "    reranker_name = \"\"\n",
    "    thresh_hold_label = \"\"\n",
    "        \n",
    "    if config[\"reranking\"]:\n",
    "        reranker_name = reranker_type + \"_\"\n",
    "        \n",
    "    if config[\"filtering\"]:\n",
    "        thresh_hold_label = \"_\" + str(thresh_hold)\n",
    "\n",
    "    output_path = f\"eval_datasets/{optimization}{config['label']}{thresh_hold_label}_{reranker_name}rag_enriched.json\"\n",
    "    # Speichere das Ergebnis als neue JSON-Datei\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "            {\"filtering\": True, \"reranking\": False, \"summarizing_prompt_engineering\": False, \"label\": \"filtering\"},\n",
    "            {\"filtering\": False, \"reranking\": True, \"summarizing_prompt_engineering\": False, \"label\": \"reranking\"},\n",
    "            {\"filtering\": False, \"reranking\": False, \"summarizing_prompt_engineering\": True, \"label\": \"summarizing_prompt_engineering\"},\n",
    "            {\"filtering\": True, \"reranking\": True, \"summarizing_prompt_engineering\": True, \"label\": \"all_combined\"},\n",
    "            {\"filtering\": True, \"reranking\": True, \"summarizing_prompt_engineering\": False, \"label\": \"filtering_reranking_combined\"},\n",
    "          ]\n",
    "\n",
    "enriched_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering \n",
    "thresh_holds = [0.25, 0.5, 0.75]\n",
    "\n",
    "for thresh_hold in thresh_holds: \n",
    "    enriched_datasets.append(enrich_eval_dataset_with_rag_responses_for_optimal_reranking(\n",
    "            eval_dataset=eval_dataset_optimal_reranking,\n",
    "            chroma_path=chroma_db_optimal_reranking,\n",
    "            config=configs[0],\n",
    "            k=15,\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            thresh_hold=thresh_hold\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e07e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking\n",
    "reranker_types = [\"bge\", \"cohere\", \"colbert\", \"monot5\", \"LLM_reranker_rankGPT\"]\n",
    "\n",
    "for reranker_type in reranker_types: \n",
    "    enriched_datasets.append(enrich_eval_dataset_with_rag_responses_for_optimal_reranking(\n",
    "            eval_dataset=eval_dataset_optimal_reranking,\n",
    "            chroma_path=chroma_db_optimal_reranking,\n",
    "            config=configs[1],\n",
    "            k=15,\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            reranker_type=reranker_type\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing\n",
    "enriched_datasets.append(enrich_eval_dataset_with_rag_responses_for_optimal_reranking(\n",
    "        eval_dataset=eval_dataset_optimal_reranking,\n",
    "        chroma_path=chroma_db_optimal_reranking,\n",
    "        config=configs[2],\n",
    "        k=6,\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21503af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All combined\n",
    "enriched_datasets.append(enrich_eval_dataset_with_rag_responses_for_optimal_reranking(\n",
    "        eval_dataset=eval_dataset_optimal_reranking,\n",
    "        chroma_path=chroma_db_optimal_reranking,\n",
    "        config=configs[3],\n",
    "        k=15,\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        reranker_type=\"LLM_reranker_rankGPT\",\n",
    "        thresh_hold=0.25\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and Reranking combined\n",
    "enriched_datasets.append(enrich_eval_dataset_with_rag_responses_for_optimal_reranking(\n",
    "        eval_dataset=eval_dataset_optimal_reranking,\n",
    "        chroma_path=chroma_db_optimal_reranking,\n",
    "        config=configs[4],\n",
    "        k=15,\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        reranker_type=\"LLM_reranker_rankGPT\",\n",
    "        thresh_hold=0.25\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069ec97",
   "metadata": {},
   "source": [
    "#### 4.3. Evaluate Retrieval & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, dataset in enumerate(enriched_datasets): \n",
    "    json_filename = dataset.split(\"/\")[-1]\n",
    "    model_name = f\"optimal_post_retrieval_reranking_{index+1}_{json_filename.replace('retrieval_eval_dataset_for_', '').replace('_rag_enriched.json', '')}\"  \n",
    "    print(json_filename)\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_optimal_chunking = {}\n",
    "generation_results_optimal_chunking = {}\n",
    "\n",
    "for index, dataset in enumerate(enriched_datasets):\n",
    "    json_filename = f\"5_optimal_reranking/{dataset.split('/')[-1]}\"\n",
    "    model_name = f\"optimal_post_retrieval_reranking_{index+1}_{json_filename.replace('5_optimal_reranking/artificial_evaluation_dataset_for_chroma_chunksize1024_overlap128_c800ccc6_optimal_reranking_', '').replace('_rag_enriched.json', '')}\"  \n",
    "    \n",
    "    print(model_name)\n",
    "\n",
    "    print(f\"\\nEvaluating {model_name} using dataset {json_filename}...\")\n",
    "\n",
    "    retrieval_result = run_retrieval_evaluation(\n",
    "        json_filename=json_filename,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    generation_result = run_generation_evaluation(\n",
    "        json_filename=json_filename,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    evaluation_results_optimal_chunking[model_name] = retrieval_result\n",
    "    generation_results_optimal_chunking[model_name] = generation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define base folder and file patterns\n",
    "folder_path = Path(\"eval_results\") / \"5_optimal_reranking\"\n",
    "pattern_retrieval = \"optimal_post_retrieval*retrieval_evaluation.csv\"\n",
    "pattern_generation = \"optimal_post_retrieval*generation_evaluation.csv\"\n",
    "\n",
    "# Find matching CSV files\n",
    "csv_retrieval_files = list(folder_path.glob(pattern_retrieval))\n",
    "csv_generation_files = list(folder_path.glob(pattern_generation))\n",
    "\n",
    "print(f\"üîç Found {len(csv_retrieval_files)} retrieval files.\")\n",
    "print(f\"üîç Found {len(csv_generation_files)} generation files.\")\n",
    "\n",
    "# Load and combine retrieval evaluation files\n",
    "df_retrieval = [pd.read_csv(f) for f in csv_retrieval_files]\n",
    "df_generation = [pd.read_csv(f) for f in csv_generation_files]\n",
    "\n",
    "# Concatenate if there is at least one file\n",
    "if df_retrieval:\n",
    "    combined_df_retrieval = pd.concat(df_retrieval, ignore_index=True)\n",
    "    output_path_retrieval = folder_path / \"combined_optimal_reranking_retrieval_evaluation.csv\"\n",
    "    combined_df_retrieval.to_csv(output_path_retrieval, index=False)\n",
    "    print(f\"‚úÖ Retrieval results saved to: {output_path_retrieval}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No retrieval CSV files found.\")\n",
    "\n",
    "if df_generation:\n",
    "    combined_df_generation = pd.concat(df_generation, ignore_index=True)\n",
    "    output_path_generation = folder_path / \"combined_optimal_reranking_generation_evaluation.csv\"\n",
    "    combined_df_generation.to_csv(output_path_generation, index=False)\n",
    "    print(f\"‚úÖ Generation results saved to: {output_path_generation}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No generation CSV files found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
