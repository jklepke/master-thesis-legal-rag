{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d9c2f6",
   "metadata": {},
   "source": [
    "# Golden Dataset Evaluation with a Standalone Language Model\n",
    "\n",
    "In this notebook, a golden dataset is processed with a standalone language model (LLM) and then automatically evaluated. The goal is to examine the model's performance in terms of accuracy, consistency with reference answers, and linguistic quality.\n",
    "\n",
    "## Procedure:\n",
    "1. **Reading in the golden dataset**  \n",
    "   The golden dataset contains predefined inputs (e.g., questions or prompts) and the expected answers (ground truth).\n",
    "\n",
    "2. **Generating model answers**  \n",
    "   The questions/prompts are passed to a standalone LLM one after the other. The model's outputs are saved.\n",
    "\n",
    "3. **Evaluation**  \n",
    "   The generated answers are compared with the reference answers, e.g., using metrics such as BLEU, ROUGE, or similarity measures.\n",
    "\n",
    "4. **Analysis**  \n",
    "   Results are evaluated quantitatively and, if necessary, qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0a7f8",
   "metadata": {},
   "source": [
    "#### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Lokale Projektmodule ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.generation_metrics import run_generation_evaluation\n",
    "from ipynb_notebooks.evaluation_datasets.generation_eval.llm_as_a_judge import run_llm_judge_parallel, run_llm_rejudge_parallel, calculate_and_visualize_scores_of_evaluation_scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd417639",
   "metadata": {},
   "source": [
    "#### 1.2 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Load environment variables. Assumes that the project directory contains a .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variables\n",
    "# Make sure to update \"OPENAI_API_KEY\" to match the variable name in your .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4816ee",
   "metadata": {},
   "source": [
    "#### 2. Enriching Golden Dataset with Generated Response from Stand-Alone LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON dataset (list of query objects)\n",
    "with open(\"eval_datasets/golden_qa_evalset_generation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# GPT-4o model configuration\n",
    "model_name = \"gpt-4o-mini\"\n",
    "temperature = 0.0  # Deterministic output for evaluation\n",
    "\n",
    "# Function to query GPT-4o with retry logic in case of API errors or rate limits\n",
    "def query_gpt4o_safe(prompt, retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Error for prompt: {prompt[:50]}... – {e}\")\n",
    "                return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run GPT queries in parallel using threads\n",
    "def generate_answers_parallel(golden_dataset_json, max_workers=5, output_json=\"golden_qa_evalset_generation_with_answers.json\"):\n",
    "\n",
    "    with open(f\"eval_datasets/{golden_dataset_json}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        entries = json.load(f)\n",
    "    \n",
    "    results = [None] * len(entries)\n",
    "\n",
    "    # Run queries in parallel threads\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_index = {\n",
    "            executor.submit(query_gpt4o_safe, entry[\"query\"]): i\n",
    "            for i, entry in enumerate(entries)\n",
    "        }\n",
    "        for future in tqdm(as_completed(future_to_index), total=len(entries), desc=\"Querying GPT-4o-mini\"):\n",
    "            idx = future_to_index[future]\n",
    "            try:\n",
    "                results[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error at index {idx}: {e}\")\n",
    "                results[idx] = \"\"\n",
    "\n",
    "    # Add the generated answers to the original entries\n",
    "    for entry, answer in zip(entries, results):\n",
    "        entry[\"generated_response\"] = answer\n",
    "        \n",
    "    output_path = f\"eval_datasets/{output_json}\"\n",
    "\n",
    "    # Write updated dataset to output file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entries, f_out, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Done. Answers written to: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd930fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset_json = \"golden_qa_evalset_generation.json\"\n",
    "\n",
    "llm_enriched_golden_dataset = generate_answers_parallel(golden_dataset_json, max_workers=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3e62a",
   "metadata": {},
   "source": [
    "#### 3 Evaluation of Generated Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e825945",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset_generation_results = run_generation_evaluation(json_filename=llm_enriched_golden_dataset.split(\"/\")[-1], \n",
    "                                               model_name=model_name, \n",
    "                                               # evaluation_mode=\"final_eval\"\n",
    "                                               ) \n",
    "display(golden_dataset_generation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be417f",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge for Claim Support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62692588",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"eval_datasets/golden_qa_evalset_generation_with_answers.json\"\n",
    "output_path = \"eval_results/golden_qa_evalset_standalone_llm_as_a_judge_results.json\"\n",
    "final_rejudge_output_path = \"eval_results/golden_qa_evalset_standalone_llm_as_a_judge_final_rejudge_results.json\"\n",
    "max_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge for Comparison and Further Justification\n",
    "\n",
    "llm_as_a_judge_first_eval_results_path = run_llm_judge_parallel(input_path=input_path, output_path=output_path, max_workers=max_workers)\n",
    "llm_as_a_judge_rejudge_results_path = run_llm_rejudge_parallel(input_path=llm_as_a_judge_first_eval_results_path, output_path=final_rejudge_output_path, max_workers=max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name_LLMaaJ_first = \"llm_as_a_judge_first_results\"\n",
    "output_file_name_LLMaaJ_rejudge = \"llm_as_a_judge_rejudge_results\"\n",
    "\n",
    "manual_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(manual_results_path, output_file_name_manual)\n",
    "llm_as_a_judge_first_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_first_eval_results_path, output_file_name_LLMaaJ_first)\n",
    "llm_as_a_judge_final_rejudge_eval_scores = calculate_and_visualize_scores_of_evaluation_scheme(llm_as_a_judge_rejudge_results_path, output_file_name_LLMaaJ_rejudge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
